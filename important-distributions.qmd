# Important distributions {#sec-important-distributions}

```{r}
#| include: false

source("_common.R")
```

::: {.chapterintro data-latex=""}
@sec-random-variables discussed general principles of random variables along with a few applications.  This chapter examines some specific random variables and their distributions which are useful models in public health and medicine
:::



```{r}
#| include: false
terms_chp_4 <- c("distributions",
                 "random variable")
```

```{r}

v_values <- c(0, 1, 2, 3, 4)
v_probs <- c(0.44, 0.19, 0.06, 0.06, 0.25)
s <- sum(v_probs)
v_mean <- sum(v_values * v_probs)
v_var <- sum((v_values - v_mean)^2 * v_probs)
v_sd <- sqrt(v_var)
c_values <- c(1200, 1220, 1240, 1260, 1280)
c_probs <- v_probs
c_mean <- 1200 + 20 * v_mean
c_var <- 3 * (20)^2 * v_var
c_sd <- sqrt(c_var)
```

## The Bernoulli and binomial distributions {#sec-bernoulli-bin-dist}

### The Bernoulli distrubution {#sec-bernoulli-dist}

Psychologist Stanley Milgram\index{Milgram, Stanley} began a series of experiments in 1963 to study the effect of authority on obedience. In a typical experiment, a participant would be ordered by an authority figure to give a series of increasingly severe shocks to a stranger. Milgram found that about 35% of people would resist the authority and stop giving shocks before the maximum voltage was reached. Over the years, additional research suggested this number is approximately consistent across communities and time.^[Find further information on Milgram's experiment at \par \ \ \hspace{0.2mm}\ \oiRedirect{textbook-milgram}{www.cnr.berkeley.edu/ucce50/ag-labor/7article/article35.htm}.]

Each person in Milgram's experiment can be thought of as a **trial** \index{trial}. Suppose that a trial is labeled a \index{success} **success** if the person refuses to administer the worst shock. If the person does administer the worst shock, the trial is a \index{failure} **failure**. The \index{probability of a success} **probability of a success** can be written as $p=0.35$. The probability of a failure is sometimes denoted with $q=1-p$.

When an individual trial only has two possible outcomes, its outcome is a \index{Bernoulli random variable} **Bernoulli random variable**. Either outcome can be labeled success, and successes are usually denoted as 1 and failures with a 0. 

Bernoulli random variables arise in many settings.  Suppose an individual joins the hypothetical HMO described in @sec-cost-of-hmo-ins and their demographic characteristics match the population used to construct the probability distribution of number of visits in a year.  If $X$ is the variable that takes on the value 1 if the new member has one or more outpatient visits and 0 if no visits, then $X$ will be a Bernoulli variable with probability of success $p$ = 1 - `r v_probs[1]` = `r 1 - v_probs[1]`.  Suppose 10 new members from this demographic joined the plan during the last year, and whether they had 1 or more outpatient visits was recorded in the sequence of ones and zeros: {0, 1, 1, 1, 1, 0, 1, 1, 0, 0}. The observation on each new member can be regarded as a Bernoulli trial, and the observed sequence as 6 successes and 4 failures. 


The \index{sample proportion} **sample proportion**, $\hat{p}$, is the sample mean of these observations:
$$
	\hat{p} = \frac{\text{number of successes}}{\text{number of trials}} = \frac{0+1+1+1+1+0+1+1+0+0}{10} = 0.6.
$$

One would expect the sample proportion $\hat{p}$ to be close to the probability $p$ of a success and that is generally the case.  But because of variability in a sequence of trials $\hat{p}$ and $p$ will almost always be different.  With a Bernoulli variable, it is possible to calculate the theoretical mean and standard deviation from its distribution.  If ${p}$ is the true probability of a success, then the mean of a Bernoulli random variable $X$ is
\begin{align*}
\mu = E[X] &= P(X=0)\times0 + P(X=1)\times1 \\
&= (1-p)\times0 + p\times 1 = 0+p = p.
\end{align*}
The variance of $X$ is:
\begin{align*}
\sigma^2 &= {P(X=0)(0-p)^2 + P(X=1)(1-p)^2} \\
&= {(1-p)p^2 + p(1-p)^2} = {p(1-p).}
\end{align*}
The standard deviation is $\sigma=\sqrt{p(1-p)}$.

The distinction between $\hat{p}$ and $p$ is important: $\hat{p}$ is a sample mean computed from $n$ observations on a Bernoulli variable; $p$ is the expected value of the Bernoulli before any observations are available.


::: {.important data-latex=""}
**Bernoulli random variable.**

If $X$ is a random variable that takes value 1 with probability of success $p$ and 0 with probability $1-p$, then $X$ is a Bernoulli random variable with mean $p$ and standard deviation $\sqrt{p(1-p)}$.

:::

In the HMO example, $X$ is a Bernoulli variable with success probability $p$ = 0.56.  In the short hand notation of statistics,  $X \sim \textrm{Bern}(0.56)$. The short hand for a general Bernoulli with success probability $p$ is $X \sim \textrm{Bern}(p)$.  

The success probability $p$ is the \index{parameter} **parameter** of the distribution, and identifies a specific Bernoulli distribution from the family of Bernoulli distributions where $p$ can be any value between 0 and 1 (inclusive). Parameters will play an important role in other distributions discussed in this chapter.

::: {.workedexample data-latex=""}

Suppose that four individuals are randomly selected to participate in Milgram's experiment. What is the chance that there will be exactly one successful trial, assuming independence between trials? Suppose that the probability of success remains 0.35.
	 

------------------------------------------------------------------------

This is  a scenario in which there is one success (i.e., one person refuses to give the strongest shock). Label the individuals as $A$, $B$, $C$, and $D$:
	
\begin{align*}
	&P(A \text{ refuses },\text{ }B \text{ shocks},\text{ }C \text{ shocks},\text{ }D \text{ shocks}) \\
	&\quad =  P(A \text{ refuses})\ P(B \text{ shocks})\ P(C \text{ shocks})\ P(D \text{ shocks}) \\
	&\quad =  (0.35)  (0.65)  (0.65)  (0.65) \\
	&\quad = (0.35)^1 (0.65)^3 \\ 
	&\quad = 0.096.
\end{align*}
	
However, there are three other possible scenarios: either $B$, $C$, or $D$ could have been the one to refuse. In each of these cases, the probability is also $(0.35)^1(0.65)^3$. These four scenarios exhaust all the ways that exactly one of these four people could refuse to administer the most severe shock, so the total probability of one success is $(4)(0.35)^1(0.65)^3 = 0.38$.

:::

### The binomial distribution {#sec-binomial-dist}

The Bernoulli distribution is unrealistic in all but the simplest of settings. However, it is a useful building block for other distributions. The \index{binomial distribution} **binomial distribution** is a  model for exactly $k$ successes in $n$ independent Bernoulli trials, each with probability of success $p$. In the Milgram example as the end of the last section, the goal was to calculate the probability of 1 success out of 4 trials, with probability of success 0.35 ($n=4$, $k=1$, $p=0.35$). 

Like the Bernoulli distribution, the binomial model has a discrete distribution, and can take on only the finite number of values 0, 1, 2, \dots, $n$.

::: {.important data-latex=""}
**The binomial model.**

A random variable $X$ is a binomial model if its value is the number of successes in $n$ Bernoulli trials meeting the following  conditions:

1. The trials are independent.
2. The number of trials is fixed and known in advance.
3. Each trial outcome can be classified as a *success* or *failure*.
4. The probability of a success, $p$, is the same for each trial.

The statistical short hand for denoting a binomial variable is  $X \sim \text{Bin}(n, p)$.

:::

The mean and standard deviation of a binomial variable can be calculated by using the Bernoulli building blocks.  Suppose $X \sim \text{Bin}(n, p)$, the total number of successes in $n$ trials with success probability $p$ .  If the random variables $X_1, X_2, \ldots, X_n$ are independent Bernoulli variables with common success probability $p$, recording 1 for a success and 0 for a failure for each of the $n$ trials, then 
$$
  X = X_1 + X_2 + \cdots + X_n.
$$
The expected value of $X$ is
\begin{align*}
 E(X) &= E(X_1) + E(X_2) + \cdots + E(X_n) \\
      &= p + p + \cdots + p \\
      &= np.
\end{align*}
The first equality follows from @eq-mean-linear-combination-rv and the second from the formula for the mean of a Bernoulli variable.

The variance of $X$ is
\begin{align*}
  \text{Var}(X) &= \text{Var}(X_1) + \text{Var}(X_2) + \cdots + \text{Var}(X_n) \\
                &= p(1 - p) + p(1 - p) + \cdots + p(1 - p) \\
                &= np(1 - p).
\end{align*}
The first equality follows from @eq-variance-linear-combination-ind-rv and the second from the variance of a Bernoulli variable.  The standard deviation is $\sqrt{\text{Var}(X)} = \sqrt{np(1 - p)}$.

It is possible to derive a formula for the probability distribution of a binomial variable generalizing the argument used in the Milgram example in @sec-bernoulli-dist; the details are outlined in the exercises.  The formula along with the mean and standard deviation are summarized here for convenience:

::: {.important data-latex=""}
**The distribution, mean and standard deviation of a binomial random variable.**

Suppose $X \sim \text{Bin}(n, p)$.  Then

* $$
  P(X = k) = \frac{n!}{k!(n-k)!}p^k(1-p)^{n-k}
  $$ {#eq-binomial-prob-formula}
  for any integer $k$ between 0 and $n$.

* The mean, variance and standard deviation of $X$ are given by
$$
\mu = E(X) = np,
$${#eq-binomial-mean}
$$
\sigma^2 = \text{Var}(X) = n p (1 - p),
$${#eq-binomial-variance}
and
$$
\sigma =  \text{standard deviation} (X) = \sqrt{n p (1 - p)}.
$${#eq-binomial-stdev}

:::

The expression $\frac{n!}{k!(n-k)!}$ arises in many settings in probability and statistics and has its own special notation:
$$
\frac{n!}{k!(n-k)!} = {n \choose k}.
$$.
It turns out to be the number of ways to choose a subset of size $k$ from a set of size $n$ and is read as $n$ choose $k$.

::: {.workedexample data-latex=""}

What is the probability that 3 of 8 randomly selected participants will refuse to administer the worst shock?


------------------------------------------------------------------------
Before using a binomial model, first check that it applies here. The number of trials is fixed ($n=8$) and each trial outcome can be classified as either success or failure. The sample is random, so the trials are independent, and the probability of success is the same for each trial. 

For the outcome of interest, $k=3$ successes occur in $n=8$ trials, and the probability of a success is $p=0.35$. Thus, the probability that 3 of 8 will refuse is given by
\begin{align*}
		P(X =3) &= { 8 \choose 3}(0.35)^3(1-0.35)^{8-3} \\
            &= \frac{8!}{3!(8-3)!}(0.35)^3(1-0.35)^{8-3} \\
		        &= (56)(0.35)^3(0.65)^5 \\
            &= 0.28.
\end{align*}

:::

::: {.workedexample data-latex=""}

What is the probability that at most 3 of 8 randomly selected participants will refuse to administer the worst shock?

------------------------------------------------------------------------
The event of at most 3 out of 8 successes is  probability of 0, 1, 2, or 3 successes. Thus, the probability that at most 3 of 8 will refuse is given by:
\begin{align*}
	P(X \leq 3) &= P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3) \\
	&= { 8 \choose 0}(0.35)^0(1-0.35)^{8-0} + { 8 \choose 1}(0.35)^1(1-0.35)^{8-1} \\
	& \qquad + { 8 \choose 2}(0.35)^2(1-0.35)^{8-2} + { 8 \choose 3}(0.35)^3(1-0.35)^{8-3} \\
	&= (1)(0.35)^0(1-0.35)^{8} + (8)(0.35)^1(1-0.35)^{7} \\
	& \qquad + (28)(0.35)^2(1-0.35)^{6} + (56)(0.35)^3(1-0.35)^{5}\\
	&= 0.706.
\end{align*}

:::

::: {.workedexample data-latex=""}

If 40 individuals were randomly selected to participate in the experiment, how many individuals would be expected to refuse to administer the worst shock? What is the standard deviation of the number of people expected to refuse?

------------------------------------------------------------------------

Using Equations [-@eq-binomial-mean] and [-@eq-binomial-stdev], the expected value (mean) is $\mu=np = 40\times 0.35 = 14$, and the standard deviation is $\sigma = \sqrt{np(1-p)} = \sqrt{40\times 0.35\times 0.65} = 3.02$.

:::

::: {.guidedpractice data-latex=""}

The probability that a smoker will develop a severe lung condition in their lifetime is about 0.30. Suppose that 5 smokers are randomly selected from the population. What is the probability that (a) one will develop a severe lung condition? (b) that no more than one will develop a severe lung condition? (c) that at least one will develop a severe lung condition? [^distributions-410]
:::

[^distributions-410]: Let $p = 0.30$; $X \sim \textrm{Bin}(5, 0.30)$. (a) $P(X=1) = {5 \choose 1}(0.30)^1(1-0.30)^{5-1} = 0.36$ (b) $P(X \leq 1) = P(X=0) + P(X=1) = {5 \choose 0}(0.30)^0(1-0.30)^{5-0} + 0.36 = 0.53$ (c) $P(X \geq 1) = 1 - P(X=0) = 1 - 0.36 = 0.83$.

## The geometric distribution {#sec-geometric-model}

::: {.important data-latex=""}
**The geometric random variable.**

The geometric distribution is a model for the waiting time until one success for a series of independent Bernoulli random variables, in which the probability of success $p$ remains constant.

:::



::: {.workedexample data-latex=""}

Recall that in the Milgram shock experiments, the probability of a person refusing to give the most severe shock is $p = 0.35$. Suppose that participants are tested one at a time until one person refuses; i.e., until the first occurrence of a successful trial. What are the chances that the first occurrence happens with the first trial? The second trial? The third?%


------------------------------------------------------------------------

The probability that the first trial is successful is simply $p = 0.35$. If the second trial is the first successful one, then the first one must have been unsuccessful. Thus, the probability is given by $(0.65)(0.35) = 0.228$.

Similarly, the probability that the first success is the third trial: $(0.65)(0.65)(0.35) = 0.148$.

This can be stated generally. If the first success is on the $n^{th}$ trial, then there are $n-1$ failures and finally 1 success, which corresponds to the probability $(0.65)^{n-1}(0.35)$.

:::

The geometric distribution from the Milgram example  is shown in @fig-bar-plot-geometric-dist-35. In general, the probabilities for a geometric distribution decrease rapidly, so that even though large waiting times to the first success are possible, they are unlikely.

```{r}
#| label: fig-bar-plot-geometric-dist-35
#| fig-cap: Bar plot of the geometric distributiuon from the Milgram example.
#| fig-alt: |
#|   coming 
#| out-width: 70%


p <- 0.35
x <- 1:100
y <- (1 - p)^(x - 1) * p

plot(x, y,
     xlim = c(0.5, 14.5),
     type = 'n',
     axes = FALSE,
     xlab = 'Number of trials',
     ylab = 'Probability')
axis(1, at = seq(2, 14, 2), cex.axis = 0.8)
par(mgp = c(2.25, 0.5, 0))
axis(2, seq(0, 0.3, 0.1), cex.axis = 0.8)
for (i in 1:14) {
  rect(x[i] - 0.4, 0,
       x[i] + 0.4, y[i],
       col = COL[1])
}
abline(h = 0)
text(14.7, 0.003, '...', col = '#444444')

```


::: {.important data-latex=""}
**The distribution, mean and standard deviation of a geometric random variable.**

A geometric random variable is used to model the waiting time to the first success in a sequence of Bernoulli trials.  If the probability of a success in one trial is $p$ and the probability of a failure is $1-p$, then the probability of finding the first success in the $k^{th}$ trial is given by
$$
P(X = k) = (1-p)^{k-1}p.
$$ {#eq-geometric-distribution}

The mean (i.e. expected value), variance, and standard deviation of this waiting time are given by
\begin{align*}
\mu &= \frac{1}{p}
	&\sigma^2&=\frac{1-p}{p^2}
	&\sigma &= \sqrt{\frac{1-p}{p^2}}
\label{geomFormulas}
\end{align*}
A geometric random variable $X$ can be expressed as $X \sim \textrm{Geom}(p)$.

:::

\begin{exercisewrap}
\begin{nexercise}
If individuals were examined until one did not administer the most severe shock, how many might need to be tested before the first success?\footnotemark{}
\end{nexercise}
\end{exercisewrap}
\footnotetext{About $1/p = 1/0.35 = 2.86$ individuals.}

\begin{examplewrap}
\begin{nexample}{What is the probability of the first success occurring within the first 4 people?}\label{marglimFirstSuccessIn4}%
This is the probability it is the first ($k=1$), second ($k=2$), third ($k=3$), or fourth ($k=4$) trial that is the first success, which represent four disjoint outcomes. Compute the probability of each case and add the separate results:
\begin{eqnarray*}
&&P(X=1, 2, 3,\text{ or }4) \\
	&& \quad = P(X=1)+P(X=2)+P(X=3)+P(X=4) \\
	&& \quad = (0.65)^{1-1}(0.35) + (0.65)^{2-1}(0.35) + (0.65)^{3-1}(0.35) + (0.65)^{4-1}(0.35) \\
	&& \quad = 0.82.
\end{eqnarray*}

Alternatively, find the complement of P(X = 0), since the described event is the complement of no success in 4 trials: $1 - (0.65)^{4}(0.35)^{0} = 0.82$.

There is a 0.82 probability that the first success occurs within 4 trials.
\end{nexample}
\end{examplewrap}

Note that there are differing conventions for defining the geometric distribution; while this text uses the definition that the distribution describes the total number of trials *including* the success, others define the distribution as the number of trials required before the success is obtained. In R, the latter definition is used. 

<!---

\index{distribution!geometric|)}

--->



## The normal distribution {#sec-normal-dist}

```{r}
library(nhanesA)
library(dplyr)
library(ggplot2)

#  BMX_J is 2021 - 2023 post-pandemic cycle
bmx  <- nhanes("BMX_L")   # Body Measures
demo <- nhanes("DEMO_L")  # Demographics

bmx_demo <- inner_join(bmx, demo, by = "SEQN")

adult_wt_ht <- bmx_demo |> 
  filter(RIDAGEYR >= 18,
         !is.na(BMXWT),
         !is.na(BMXHT),
         !is.na(RIAGENDR),
         BMXWT > 0,
         BMXHT > 0) |> 
  select(BMXWT, BMXHT, RIAGENDR) 

female_heights <- adult_wt_ht |> 
  filter(RIAGENDR == "Female")

male_heights <- adult_wt_ht |> 
  filter(RIAGENDR == "Male")

```

```{r}

n_female <- nrow(female_heights)
mean_female <- round(mean(female_heights$BMXHT), 2)
med_female <- round(median(female_heights$BMXHT), 2)
sd_female <- round(sd(female_heights$BMXHT, 2))

n_male <- nrow(male_heights)
mean_male <- round(mean(male_heights$BMXHT), 2)
med_male <- round(median(male_heights$BMXHT), 2)
sd_male <- round(sd(male_heights$BMXHT, 2))

```

e;;

### Definition of the normal model {#sec-definition-normal-model}


@fig-height-density-histograms-by-sex shows density histograms for the heights in cm of `r n_female` females (upper figure) and `r n_male` males (lower figure) age 18 or older. The data come from the National Health and Nutrition Examination Survey (NHANES) and were used in @sec-continuous-rv.  In these plots, the heights of the bars represent the probability of the observations found in each interval of width 2cm.  In the data for the females, the probability of a height  between 160 and 162cm is approximately 0.058, and is approximately 0.056 for heights between 162 and 164cm, so the probability of an observed height  between 160 and 164cm is approximately 0.114. 

The shape of the two distributions is characteristic of many measurements made on human populations -- the histograms are bell-shaped, symmetric, unimodal and appear to have few outliers, although that would have to be confirmed with a box plot.  Because of the symmetry and lack of outliers, the mean and medians will be similar.  The female heights have mean `r mean_female` and median `r med_female`; the corresponding values for the male heights are `r mean_male` and `r med_male`.  



```{r height-histograms-by-sex}
#| label: fig-height-density-histograms-by-sex
#| fig-cap: |
#|   Histograms of the heights of adult females and males 
#| fig-subcap:
#|   - Female heights (cm)
#|   - Male heights (cm)
#| fig-alt: | 
#|   To be written
#| fig-asp: 0.4
#| layout-ncol: 1

adult_wt_ht |> filter(RIAGENDR == "Female") |> 
  ggplot(aes(x = BMXHT)) +
  geom_histogram(aes(y = after_stat(density)),
                 breaks = seq(140, 200, 2.0),
                 closed = "right") +
  labs(x = "Height (cm)", y = "Density") +
  scale_x_continuous(
    breaks = seq(140, 200, 20)) 

adult_wt_ht |> filter(RIAGENDR == "Male") |> 
  ggplot(aes(x = BMXHT)) +
  geom_histogram(aes(y = after_stat(density)),
                 breaks = seq(140, 200, 2.0),
                 closed = "right") +
  labs(x = "Height (cm)", y = "Density") +
  scale_x_continuous(
    breaks = seq(140, 200, 20)) 

```

The \index{normal model} **normal model** is often used to represent the population distribution of measurements with the characteristics seen in @fig-height-density-histograms-by-sex.

::: {.important data-latex=""}
**The normal model.**

A random variable $X$ follows a normal model if it can take on all values, and its distribution is a bell-shaped curve that is symmetric, unimodal and with very few outliers. 

:::

A normal random variable $X$ is a continuous random variable because its values are not limited to a finite discrete set of values. 

### The normal density function {#sec-normal-density}

The histograms in @fig-height-density-histogram-normal add normal density functions (in red) to each of the histograms in @fig-height-density-histograms-by-sex.   The normal density functions are the distributions of normal random variable that model for heights of all adult women or men in the US population.  The two densities have their peaks at different values (males tend to be taller than women but have roughly equal spread.


```{r height-density-histogram-normal}
#| label: fig-height-density-histogram-normal
#| fig-cap: |
#|   A density histogram of the heights of US adults, with a normal density added in red.
#| fig-subcap:
#|   - Female heights (cm)
#|   - Male heights (cm)
#| fig-alt: | 
#|   To be written
#|   
#| fig-asp: 0.4
#| layout-ncol: 1

adult_wt_ht |> filter(RIAGENDR == "Female") |> 
  ggplot(aes(x = BMXHT)) +
  geom_histogram(aes(y = after_stat(density)),
                 breaks = seq(140, 200, 2.0),
                 closed = "right") +
  stat_function(
    fun = function(Height) dnorm(Height, mean = mean_female, sd = sd_female),
    color = "red", linewidth = 1 ) +
  labs(x = "Adult female height (cm)", y = "Density") +
  scale_x_continuous(
    breaks = seq(140, 200, 10))
 
adult_wt_ht |> filter(RIAGENDR == "Male") |> 
  ggplot(aes(x = BMXHT)) +
  geom_histogram(aes(y = after_stat(density)),
                 breaks = seq(140, 200, 2.0),
                 closed = "right") +
  stat_function(
    fun = function(Height) dnorm(Height, mean = mean_male, sd = sd_male),
    color = "red", linewidth = 1 ) +
  labs(x = "Adult male height (cm)", y = "Density") +
  scale_x_continuous(
    breaks = seq(140, 200, 10))
 
```

::: {.important data-latex=""}
**The normal density function.**

The normal density function is a symmetric, unimodal, bell-shaped curve that describes the distribution of a normal random variable $X$.  The total area under a normal density function is 1.

The normal density is characterized by two parameters: the mean $\mu$ and standard deviation $\sigma$ of $X$.   The statistical short hand for denoting a normal random variable is  $X \sim N(\mu, \sigma).$  When $Z \sim N(0, 1)$ it is called a standard normal variable. 

:::

The normal density function characterizes a family of normal random variables indexed by the two parameters $\mu$ and $\sigma$.  When $\mu$ changes, the center of the distribution shifts right or left; when $\sigma$ changes the  distribution changes its spread. @fig-normal-densities shows 4 normal density functions with means 100 or 150, and standard deviations 25 or 40.

```{r normal-densities}
#| label: fig-normal-densities
#| fig-cap: |
#|   Four normal density functions
#| fig-alt: | 
#|   To be written
#|   
#| fig-asp: 0.4
# parameters
means <- c(100, 150)
sds   <- c(25, 40)

# build a data frame with all combinations
params <- expand.grid(mean = means, sd = sds)

# create x values wide enough to cover all curves
x <- seq(means[1] - 3*sds[2], means[2] + 3*sds[2], length.out = 500)

# compute densities for each combination
densities <- do.call(rbind, lapply(1:nrow(params), function(i) {
  data.frame(
    x = x,
    y = dnorm(x, mean = params$mean[i], sd = params$sd[i]),
    mean = params$mean[i],
    sd = params$sd[i]
  )
}))

# combine mean and sd into a label
densities$label <- with(densities, paste0("μ=", mean, ", σ=", sd))

# plot
ggplot(densities, aes(x = x, y = y, color = label)) +
  geom_line(linewidth = 1.1) +
  labs(
    x = "x", y = "Density", color = "Parameters"
  ) 
#  theme_minimal(base_size = 14)


```

@fig-standard-normal-density is a plot of the density for a standard normal random variable.  In principle any value of a normal random variable is possible, values of a standard normal smaller than -4 or larger than 4 are unlikely

```{r standard-normal-density}
#| label: fig-standard-normal-density
#| fig-cap: |
#|   The density function for a standard normal variable $Z$
#| fig-alt: | 
#|   To be written
#|   
#| fig-asp: 0.4
# parameters

library(ggplot2)

# create x and y values for the standard normal
x <- seq(-4, 4, length.out = 500)
df <- data.frame(x = x, y = dnorm(x))  # mean=0, sd=1 by default

ggplot(df, aes(x = x, y = y)) +
  geom_line(linewidth = 1.2, color = "steelblue") +
  labs(
    x = "Z-value",
    y = "Density"
  ) 

```



### Standardizing with $z$-scores {#sec-z-scores}

When data are approximately normally distributed, the $Z$-score of an observation quantifies how far the observation is from the mean, in units of standard deviation(s). If $x$ is an observation from a distribution $N(\mu, \sigma)$, the Z-score is defined as:
\begin{align*}
	Z = \frac{x-\mu}{\sigma}.
\end{align*}

An observation equal to the mean has a $Z$-score of 0. Observations above the mean have positive $Z$-scores, while observations below the mean have negative $Z$-scores. For example, if an observation is one standard deviation above the mean, it has a $Z$-score of 1; if it is 1.5 standard deviations below the mean, its $Z$-score is -1.5. 

$Z$-scores can be used to identify which observations are more extreme than others, and are especially useful when comparing observations from different normal distributions. One observation $x_1$ is said to be more unusual than another observation $x_2$ if the absolute value of its z-score is larger than the absolute value of the other observation's Z-score: $|z_1| > |z_2|$. In other words, the further an observation is from the mean in either direction, the more extreme it is. 

$Z$-scores are also used to compare observations from normal distributions that are measured on different scales.

::: {.workedexample data-latex=""}

The SAT and the ACT are two standardized tests commonly used for college admissions in the United States. The distribution of test scores are both nearly normal. For the SAT, $N(1500, 300)$; for the ACT, $N(21, 5)$. While some colleges request that students submit scores from both tests, others allow students the choice of either the ACT or the SAT. Suppose that one student scores an 1800 on the SAT (Student A) and another scores a 24 on the ACT (Student B). A college admissions officer would like to compare the scores of the two students to determine which student performed better.

------------------------------------------------------------------------

Calculate a $Z$-score for each student; i.e., convert $x$ to $Z$.
		
Using $\mu_{SAT}=1500$, $\sigma_{SAT}=300$, and $x_{A}=1800$, find Student A's $Z$-score:
\begin{align*}
	Z_{A} = \frac{x_{A} - \mu_{SAT}}{\sigma_{SAT}} = \frac{1800-1500}{300} = 1.
\end{align*}

For Student B:
\begin{align*}
	Z_{B} = \frac{x_{B} - \mu_{ACT}}{\sigma_{ACT}} = \frac{24 - 21}{5} = 0.6.
\end{align*}

Student A's score is 1 standard deviation above average on the SAT, while Student B's score is 0.6 standard deviations above the mean on the ACT. As illustrated in @fig-sat-act-z-score, Student A's score is more extreme, indicating that Student A has scored higher with respect to other scores than Student B.

:::



```{r sat-act-z-score}
#| label: fig-sat-act-z-score
#| fig-cap: |
#|    Scores of Students A and B plotted on the distributions of SAT and ACT scores.
#| fig-subcap:
#|      - Distribution of SAT scores
#|      - Distribution of ACT scores
#| fig-alt: | 
#|   To be written
#| fig-asp: 0.4
#| layout-ncol: 1


library(openintro)
data(COL)

set.seed(1)


# _____ Curve 1 _____ #
m <- 1500
s <- 300
X <- m + s * seq(-4, 4, 0.01)
Y <- dnorm(X, m, s)
plot(X, Y,
     type = 'l',
     axes = FALSE,
     xlim = m + s * c(-2.7, 2.7),
     xlab = "",
     ylab = "")
axis(1, at = m + s * (-3:3))
abline(h = 0)
lines(c(m, m),
      dnorm(m, m, s) * c(0.01, 0.99),
      lty = 2,
      col = '#EEEEEE')
lines(c(m, m) + s,
      dnorm(m + s, m, s) * c(0.01, 1.25),
      lty = 2, col = COL[1])
text(m + s + 60,
     dnorm(m + s, m, s) * 1.25,
     'Student A',
     pos = 3,
     col = COL[1])


# _____ Curve 2 _____ #
# par(mar = c(2, 0, 1, 0))
m <- 21
s <- 5
X <- m + s * seq(-4, 4, 0.01)
Y <- dnorm(X, m, s)
plot(X, Y,
     type = 'l',
     axes = FALSE,
     xlim = m + s * c(-2.7, 2.7),
     xlab = "",
     ylab = "")
axis(1, at = m + s * (-3:3))
abline(h = 0)
lines(c(m, m),
      dnorm(m, m, s) * c(0.01, 0.99),
      lty = 2,
      col = '#EEEEEE')
lines(c(m, m) + 3,
      dnorm(m + 3, m, s) * c(0.01, 1.2),
      lty = 2,
      col = COL[1])
text(m + 3,
     dnorm(m + 3, m, s) * 1.05,
     'Student B',
     pos = 4,
     col = COL[1])




```

::: {.important data-latex=""}
**The Z-score.**

The $Z$-score of an observation quantifies how far the observation is from the mean, in units of standard deviation(s). The $Z$-score for an observation $x$ that follows a distribution with mean $\mu$ and standard deviation $\sigma$ can be calculated using
\begin{align*}
z = \frac{x-\mu}{\sigma}.
\end{align*}

:::

::: {.workedexample data-latex=""}

How high would a student need to score on the ACT to have a score equivalent to Student A's score of 1800 on the SAT?

------------------------------------------------------------------------

As shown in the above example, a score of 1800 on the SAT is 1 standard deviation above the mean. ACT scores are normally distributed with mean 21 and standard deviation 5. To convert a value from the standard normal curve (Z) to one on a normal distribution $N(\mu, \sigma)$:

\begin{align*}
x = \mu + Z\sigma.
\end{align*}

Thus, a student would need a score of $21 + 1(5) = 26$ on the ACT to have a score equivalent to 1800 on the SAT. 

:::

::: {.guidedpractice data-latex=""}

Systolic blood pressure (SBP) for adults in the United States aged 18-39 follow an approximate normal distribution, $N(115, 17.5)$. As age increases, systolic blood pressure also tends to increase. Mean systolic blood pressure for adults 60 years of age and older is 136 mm Hg, with standard deviation 40 mm Hg. Systolic blood pressure of 140 mm Hg or higher is indicative of hypertension (high blood pressure). 
(a) How many standard deviations away from the mean is a 30-year-old with systolic blood pressure of 125 mm Hg? (b) Compare how unusual a systolic blood pressure of 140 mm Hg is for a 65-year-old, versus a 30-year-old. [^important-distributions-310]
:::

[^important-distributions-310]: a. Calculate the $z$-score: $\frac{\overline{x} - \mu}{\sigma} = \frac{125 - 115}{17.5} = 0.571$. A 30-year-old with systolic blood pressure of 125 mm Hg is about 0.6 standard deviations above the mean. (b) For $x_1=140$ mm Hg: $z_1 = \frac{x_1 - \mu}{\sigma} = \frac{140 - 115}{17.5} = 1.43$. For $x_2=140$ mm Hg: $z_2 = \frac{x_2 - \mu}{\sigma} = \frac{140 - 136}{40} = 0.1$. While an SBP of 140 mm Hg is almost 1.5 standard deviations above the mean for a 30-year-old, it is only 0.1 standard deviations above the mean for a 65-year-old.


### The empirical rule{#sec-empirical-rule}

The empirical rule (also known as the 68-95-99.7 rule) states that for a normal distribution, almost all observations will fall within three standard deviations of the mean. Specifically, 68\% of observations are within one standard deviation of the mean, 95\% are within two SD's, and 99.7\% are within three SD's. 

```{r empirical-rule-normal}
#| label: fig-empirical-rule-normal
#| fig-cap: |
#|   Probabilities for falling within 1, 2, and 3 standard deviations of the mean in a normal distribution.
#| fig-alt: | 
#|   To be written
#|   
#| fig-asp: 0.6

X <- seq(-4, 4, 0.01)
Y <- dnorm(X)
plot(X, Y,
     type = 'n',
     axes = FALSE,
     xlim = c(-3.2, 3.2),
     ylim = c(0, 0.4))
abline(h = 0, col = COL[6])
at <- -3:3
labels <- expression(mu - 3 * sigma,
                     mu - 2 * sigma,
                     mu - sigma,
                     mu,
                     mu + sigma,
                     mu + 2 * sigma,
                     mu + 3 * sigma)
axis(1, at, labels)
for (i in 3:1) {
  these <- (i - 1 <= X & X <= i)
  polygon(c(i - 1, X[these], i),
          c(0, Y[these], 0),
          col = COL[i],
          border = COL[i])
  these <- (-i <= X & X <= -i + 1)
  polygon(c(-i, X[these], -i + 1),
          c(0, Y[these], 0),
          col = COL[i],
          border = COL[i])
}

# _____ Label 99.7 _____ #
arrows(-3, 0.03,
       3, 0.03,
       code = 3,
       col = '#444444',
       length = 0.15)
text(0, 0.02, '99.7%', pos = 3)

# _____ Label 95 _____ #
arrows(-2, 0.13,
       2, 0.13,
       code = 3,
       col = '#444444',
       length = 0.15)
text(0, 0.12, '95%', pos = 3)

# _____ Label 68 _____ #
arrows(-1, 0.23,
       1, 0.23,
       code = 3,
       col = '#444444',
       length = 0.15)
text(0, 0.22, '68%', pos = 3)

lines(X, Y, col = '#888888')
abline(h = 0, col = '#AAAAAA')


```

While it is possible for a normal random variable to take on values 4, 5, or even more standard deviations from the mean, these occurrences are extremely rare if the data are nearly normal. For example, the probability of being further than 4 standard deviations from the mean is about 1-in-30,000.



### Calculating normal probabilities {#sec-calculating-normal-probabilities}

There are two main types of problems that involve the normal distribution: calculating probabilities from a given value (whether $X$ or $Z$), or identifying the observation that corresponds to a particular probability. Normal probabilities can be calculated using either software or tables of the normal distribution in @sec-normal-probability-table.  The tables in @sec-normal-probability-table are sometimes useful in a classroom setting but are never used in practice.   Examples using tables of the normal distribution are given in @sec-normal-probability-table. 

There are many software resources for calculating normal probabilities, including pocket calculators like the TI-84 or online programs like the [Desmos graphing calculator](https://www.desmos.com/calculator).  This section illustrates functions in the  R statistical software to compute normal probabilities.  The same examples used here are also used in @sec-normal-probability-table to illustrate the use of the normal table.

In R, the  function **pnorm** calculates the area under a normal curve to the left of a specified value, so it calculates the probability of observing a value less than or equal to the specified value. The normal curve is specified by its mean and standard deviation.  When the mean and standard deviation are not given, a mean and standard deviation 0 and 1 are used. 


::: {.workedexample data-latex=""}

Based on the data in  @sec-definition-normal-model the heights of US adult females is approximately normally distributed with mean 160cm and standard deviation 7cm.  Use the function pnorm to find percentile of the height distribution for a woman 167cm tall.

------------------------------------------------------------------------

The percentile is the percentage of adult women who are the same height or shorter, and it will be the size of the shaded area in the normal curve in @fig-height-below-167-example


```{r}
#| label: fig-height-below-167-example
#| fig-cap: Normal distribution for the heigts of adult US women.
#| fig-alt: A normal distribution is shown that is centered at 160cm with a standard deviation of 200cm. The region to the left of 167 (where Z equals 1) is shaded, which appears to represent about 80\% of the area under the distribution.

normTail(160, 7,
         L = 167,
         col = COL[1],
         cex.axis = 0.6)

```

The function pnorm can be used to calculate the size of the shaded area.  In pnorm, $q$ denotes the observed value, labeled a quantile in R.

```{r echo=TRUE}
pnorm(q = 167, mean = 160, sd = 7)

```
This woman  is in the $84^{th}$ percentile of heights.
:::

::: {.workedexample data-latex=""}

Use a $Z$-score and the standard normal distribution to calculate the percentile in the above example.

------------------------------------------------------------------------

For height 167cm, 
$$
Z = \frac{167 - 160}{7} = 1.
$$
The corresponding area under a standard normal curve is shown in @fig-height-below-1sd.

```{r fig-height-below-1sd}
#| label: fig-height-below-1sd
#| fig-cap: Standard normal distribution with shaded area below Z = 1.
#| fig-alt: A standard normal distribution is shown that is centered at 0 with a standard deviation 1. The region to the left of 1 (where Z equals 1) is shaded, which appears to represent about 80\% of the area under the distribution.

normTail(0, 1,
         L = 1,
         col = COL[1],
         cex.axis = 0.6)

```
The size of area can be calculated with the default version of pnorm.


```{r echo=TRUE}
pnorm(1)

```

:::


Tail areas can be used to calculate the probability of an observation falling in an interval with specified endpoints.



::: {.workedexample data-latex=""}

What is the probability that an adult women's  height is between 158 and 169cm?

------------------------------------------------------------------------

First, draw the figure that represents the probability.  The area of interest is an interval, not a tail area.

```{r}
#| label: fig-area-between-158-169
#| fig-cap: Normal density for adulst women's height showing the area between 158 and 168cm. 
#| fig-alt: A normal distribution is shown centered at 160 with standard deviation 7.  The area under the density curve between 158cm and 169cm is shaded.

normTail(160, 7,
         M = c(158, 169),
         col = COL[1],
         axes = FALSE)
labels <- round(c(158, 169), 2)
axis(1, labels, cex.axis = 0.8)

```

To find the middle area, find the area to the left of 169; from that area, subtract the area to the left of 158.
	
First, convert to $Z$-scores:
	
$$
	Z_{169} = \dfrac{x-\mu}{\sigma} = \dfrac{169 - 160}{7} = 1.29, \qquad Z_{158} = \dfrac{x-\mu}{\sigma} = \dfrac{158-160}{7} = -0.286.
$$
 
 Now use pnorm to calculate the tail areas corresponding to these $Z$-scores.
 
```{r echo=TRUE}
pnorm(1.29) - pnorm(-0.286)
```

The steps are shown graphically in 


```{r}
#| label: fig-diff-tail-areas-169-158
#| fig-cap: Standard normal densities depicting the operation to calculate the area under a normal density in an interval. 
#| fig-alt: The figure show graphically that the area under the normal curve in an interval is obtained by subtracting two tail areas.  Three standard normal densities with mean 0 and standard deviation 1 are shown, side by side.  The density on the left shows the shaded area in the left tail less than or equal to Z = 1.29, with the probability of that area, 0.901, written above the density curve.  The middle density shows the shaded area in the left tail less than or equal to Z = -0.286, with probability value 0.388 above it. The density on the right shows the shaded area between Z = -0.286 and Z = 1.29.  The probabilty of the shaded area, 0.518, is written above the density.
#| 

AddShadedPlot <- function(x, y, offset,
                          shade.start = -8,
                          shade.until = 8) {
  lines(x + offset, y)
  lines(x + offset, rep(0, length(x)))
  these <- which(shade.start <= x & x <= shade.until)
  polygon(c(x[these[1]], x[these], x[rev(these)[1]]) + offset,
          c(0, y[these], 0),
          col = COL[1])
  lines(x + offset, y)
}
AddText <- function(x, text) {
  text(x, 0.549283, text)
}

X <- seq(-3.2, 3.2, 0.01)
Y <- dnorm(X)

plot(X, Y,
     type = 'l',
     axes = FALSE,
     xlim = c(-3.4, 16 + 3.4),
     ylim = c(0, 0.622),
     xlab = "",
     ylab = "")

AddShadedPlot(X, Y, 0, -8, 1.21)
AddText(0, format(0.901 , scientific = FALSE)[1])

AddShadedPlot(X, Y, 8, -8, -0.3)
AddText(8, format(0.387, scientific = FALSE)[1])

AddShadedPlot(X, Y, 16, -0.3, 1.21)
AddText(16, format(0.514, scientific = FALSE)[1])

lines(c(3.72, 4.28), rep(0.549283, 2), lwd = 1)
lines(c(3 + 3, 2*3-3.5), c(0.2, 0.2), lwd = 3)

text(12, 0.549283,
     ' = ')
segments(rep(11, 2), c(0.17, 0.23), rep(13, 2), lwd = 3)


```

:::


The R function **qnorm** can be used to find the value of an observation that corresponds to a given percentile.  In qnorm, $p$ denotes required percentile.


::: {.workedexample data-latex=""}

Find the height that marks the 80^th^ percentile of the distribution of heights of adult women in the United States.
 
------------------------------------------------------------------------

Use qnorm with the specified percentile, and the mean and standard deviation of the normal distribution.

```{r echo=TRUE}
qnorm(p = 0.80, mean = 160, sd = 7)

```
The 80^th^ percentile for the distribution of heights is 166cm.  In other words, 80% of US women are as short or shorter than 166cm; 20\% are taller than 166cm.

When the mean and standard deviation are not specificed, qnorm assumes a standard normal and  returns the $Z$-score for that percentile.

```{r echo=TRUE}
qnorm(p = 0.80)

```
In this problem, the $Z$-score can then be converted back to the original scale, using
the $Z$-score and solving for $x$:
  
  $$
  Z = 0.84 = \frac{x - 160}{7}, 
  $$
  so 
  $$
  x = (0.84)(7) + 160 = 166.
  $$
  Solving for $x$ yields $x$ = 166cm. 
  
:::


## The Poisson distribution {#sec-poisson-dist}

The **Poisson distribution** \index{distribution!Poisson} is a discrete distribution used to calculate probabilities for the number of occurrences of a rare event.  In technical terms, it is used as a model for count data. For example, historical records of hospitalizations in New York City indicate that among a population of approximately 8 million people, 4.4 people are hospitalized each day for an acute myocardial infarction (AMI), on average.  A histogram of showing the distribution of the number of AMIs per day on 365 days for NYC is shown in Figure~\ref{amiIncidencesOver100Days}.^[These data are simulated. In practice, it would be important to check for an association between successive days.]



```{r}
#| label: fig-ami-incidences-over-100-days
#| fig-cap: A histogram of the number of people hospitalized for an AMI on 365 days for NYC, 
#|     as simulated from a Poisson distribution with mean 4.4.   
#|  
#| fig-alt: A histogram is shown for "AMI Events (by Day)". There are 11 non-zero values shown - a frequency of 
#|     about 15 at a value of 1, a frequency of 50 at 2, 70 at 3, 85 at 4, 55 at 5, 45 at 6, 
#|     25 at 7, 20 at 8, 5 at 9, 5 at 10, and a frequency of about 2 at 11.    
#| 
set.seed = 1
r <- 200 / 10^6
N <- 8 * 10^6
n <- 365
x <- rpois(n, r * N / 365)

histPlot(x,
         breaks = (0:max(2 * x + 1)) / 2 - 0.25,
         axes = FALSE,
         col = COL[1],
         xlab = "AMI events (by day)")
at     <- 0:1000
# labels <- rep("", length(at))
# axis(1, at = at, labels = labels, tcl = -0.18)
axis(1, at = seq(0, 1000, 5), tcl = -0.35)
axis(2, at = seq(0, 1000, 20))



```



## Distributions related  to  Bernoulli trials {#sec-dist-related-to-bernoulli}

## Distributions for pairs of random variables {#sec-distributions-pairs-rv}


## Chapter review {#sec-chp4-review}

### Summary

Chapter summary here


```{r}
#| label: tbl-terms-chp-4
#| tbl-cap: Terms introduced in this chapter.
#| tbl-pos: H
make_terms_table(terms_chp_4)
```

\clearpage

## Exercises {#sec-chp4-exercises}

