# Random variables {#sec-random-variables}



```{r}
#| include: false

source("_common.R")
options(warn = 1)
```

:::{.underconstruction data-latex="}

Incomplete draft, possible additions and corrections pending

:::




::: {.chapterintro data-latex=""}
Data are the result of a study. The study might have been an experiment or observational study, and with data collected using the methods discussed in @sec-collecting-data. **Random variables** \index{random variable} connect the important concepts of recording measurements and probability: a random variable assigns numerical values to the outcome of a random phenomenon and the distribution of the variable reflects the variability in outcomes. 

Random variables are usually written with a capital letter such as $X$, $Y$, or $Z$ and are characterized as either discrete or continuous. This chapter outlines general properties of random variables and their distributions. The next chapter discusses some specific random variables that often arise in practice.
:::

```{r}
#| include: false
terms_chp_4 <- c("random variable (RV)")
```

## Discrete random variables {#sec-discrete-rv}

If a coin is tossed three times, the outcome of the process is the sequence of observed heads and tails. One such outcome might be TTH: tails on the first two tosses, heads on the third. If the random variable $X$ is the number of heads for the three tosses, $X=1$; if $Y$ is the number of tails, then $Y=2$. For the sequence THT, only the order has changed, but the values of $X$ and $Y$ remain the same. For the sequence HHH, however, $X=3$ and $Y=0$. The values of $X$ and $Y$ will vary each time the process of tossing the coin three times is repeated. 

```{r three-coin-tosses}
#| label: fig-coin-toss
#| out-width: 70%
#| fig-cap: 	Possible outcomes for number of heads in three tosses of a coin.
#| fig-alt:   draft alt caption here
#| fig-pos: H
library(ggplot2)
library(dplyr)
library(tidyr)

# plot constructed by iterating with ChatGPT 5.0
#--- helper to turn a 3×3 character matrix into a tidy data frame -------------
grid_to_df <- function(mat, panel_label){
  stopifnot(all(dim(mat) == c(3,3)))
  as.data.frame(mat, stringsAsFactors = FALSE) |>
    mutate(y = 3:1) |>
    pivot_longer(-y, names_to = "x", values_to = "face") |>
    mutate(x = match(x, colnames(mat)), 
           panel = panel_label)
}

# Panel 1 (three T’s)
p1 <- tibble(
  panel = "X = 0",
  x = 1:3,
  y = 1,
  face = "T"
)

# Panel 2 and 3 (3×3 grids)
grid2 <- matrix(c(
  "T","T","H",
  "T","H","T",
  "H","T","H"
), nrow = 3, byrow = TRUE,
dimnames = list(NULL, c("x1","x2","x3")))

grid3 <- matrix(c(
  "T","H","H",
  "H","T","H",
  "H","H","T"
), nrow = 3, byrow = TRUE,
dimnames = list(NULL, c("x1","x2","x3")))

p2 <- grid_to_df(grid2, "X = 1")
p3 <- grid_to_df(grid3, "X = 2")

# Panel 4 (three H’s)
p4 <- tibble(
  panel = "X = 3",
  x = 1:3,
  y = 1,
  face = "H"
)

# Combine all panels
df <- bind_rows(p1, p2, p3, p4) |>
  mutate(panel = factor(panel, levels = c("X = 0","X = 1","X = 2","X = 3")),
         y_plot = y)

# Final plot
ggplot(df, aes(x = x, y = y_plot)) +
  # white footer bar
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = 0, ymax = 0.6, fill = "white") +
  # coin outlines
  geom_point(shape = 21, size = 18, stroke = 1.2, fill = "white", color = "#2E6FDF") +
  # H/T letters
  geom_text(aes(label = face), size = 6, color = "#2E6FDF", fontface = "bold") +
  # facet labels BELOW the panels
  facet_wrap(~panel, nrow = 1, strip.position = "bottom") +
  scale_x_continuous(limits = c(0.5, 3.5), breaks = 1:3) +
  scale_y_continuous(limits = c(0, 3.6), breaks = 1:3) +
  coord_fixed() +
  theme_minimal(base_size = 13) +
  theme(
    panel.background = element_rect(fill = "#F3F3F3", color = NA),
    panel.spacing = unit(0, "pt"),              # replaces deprecated panel.margin
    strip.background = element_rect(fill = "white", color = "black"),
    strip.text = element_text(size = 14, face = "bold", color = "black"),
    strip.placement = "outside",
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    plot.margin = margin(0, 0, 0, 0),
    panel.grid = element_blank()
  )

```

The probabilities  assigned to the outcomes in a random phenomenon are used to assign probabilities to values of a random variable. Using independence, $P(\text{HHH}) = (1/2)^3 = 1/8$. Since $X$ in the above example can only be three if the three tosses are all heads, $P(X=3) = 1/8$. The distribution of a random variable is the collection of probabilities for all of the variable's possible values. @fig-coin-toss shows the eight possible outcomes when a coin is tossed three times: TTT, HTT, THT, TTH, HHT, HTH, THH, HHH. For the first set of tosses, $X = 0$; for the next three, $X=1$, then $X=2$ for the following three tosses and $X=3$ for the last set (HHH).

Using independence again, each of the 8 outcomes has probability 1/8, so $P(X = 0) = P(X = 3) = 1/8$ and $P(X = 1) = P(X = 2) = 3/8$. @tbl-dist-coin-tossing shows the probability distribution for $X$. Probability distributions for random variables follow the rules for probability; for instance, the probabilities must be between 0 and 1 and their sum must be 1. The possible outcomes of $X$ are labeled with a corresponding lower case letter $x$ and subscripts. The values of $X$ are $x_1 = 0$, $x_2 = 1$, $x_3 = 2$, and $x_4 = 3$; these occur with probabilities $1/8$, $3/8$, $3/8$ and $1/8$.

|     $i$      |  1  |  2  |  3  |  4  |      Total |
|:------------:|:---:|:---:|:---:|:---:|-----------:|
|    $x_i$     |  0  |  1  |  2  |  3  |         -- |
| $P(X = x_i)$ | 1/8 | 3/8 | 3/8 | 1/8 | 8/8 = 1.00 |

: Tabular form for the distribution of the number of heads in three coin tosses. {#tbl-dist-coin-tossing}

**Bar plots** can be used to show the distribution of a random variable. @fig-bar-plot-coin-tossing is a bar plot of the distribution of $X$ in the coin tossing example. When bar plots are used to show the distribution of a dataset, the heights of the bars show either the frequency or relative frequency of each observation of the variable in the dataset. In contrast, bar heights for a probability distribution show the probabilities of possible values of a random variable before any observations have been made.

```{r bar-plot-coin-tossing}
#| label: fig-bar-plot-coin-tossing
#| fig-cap: Bar plot of the distribution of the number of heads in three coin tosses.
#| fig-alt: |
#|   coming 
#| fig-width: 4

# Create the data frame
x.df <- data.frame(
  x.values = c(0, 1, 2, 3),
  x.probs = c(1/8, 3/8, 3/8, 1/8)
)

ggplot(x.df, aes(x = x.values, y = x.probs)) +
  geom_col(fill = IMSCOL["blue", "full"], 
           width = 0.5,
           position = "dodge") +
  labs(
    x = "Number of Heads (X)",
    y = "Probability"
  ) +
  theme(
    axis.title.x = element_text(size = 8),  # X-axis title size
    axis.title.y = element_text(size = 8),  # Y-axis title size
    axis.text.x  = element_text(size = 8),  # X-axis tick labels
    axis.text.y  = element_text(size = 8)   # Y-axis tick labels
  )
```

$X$ is an example of a **discrete random variable** \index{discrete random variable} -- a variable taking on a finite number of values  -- and its distribution.[^random-variables-1]

[^random-variables-1]: Some discrete random variables have an infinite number of possible values, such as all the non-negative integers.

```{r}
#| include: false
terms_chp_4 <- c(terms_chp_4,
                 "discrete RV",
                 "bar plots")
```

### Distribution of a discrete random variable

::: {.important data-latex=""}
**Definition of the distribution of a discrete random variable.**

The distribution of a discrete random variable $X$ is the set of its possible values $x_1, x_2, \dots, x_k$ and the associated probabilities $P(x_1), P(x_2), \ldots , P(x_k)$. It must satisfy the following conditions:

-   $0 \leq P(x_i) \leq 1.$

-   $P(x_1) + P(x_2) + \dots + P(x_k) = \sum_{i=1}^{k} P(X=x_i) = 1$

Distributions of discrete random variables are usually specified in tables, such as @tbl-dist-coin-tossing, or bar plots such as @fig-bar-plot-coin-tossing.
:::

::: {.workedexample data-latex=""}
Suppose a random process consists of rolling two six-sided dice. (a) Create a table showing the possible outcomes of the roll and the probability distribution of the sum of the faces. (b) Show the distribution in a bar graph.

------------------------------------------------------------------------

(a) The first column of @tbl-sum-two-dice shows all 36 possible values of the roll of two dice, the sum for each value, and the associated probability of the value of the sum under the assumption that all 36 outcomes of the roll are equally likely.

| Possible values of the two dice          | Sum | Probability  |
|:-----------------------------------------|:----|:-------------|
| (1,1)                                    | 2   | 1/36 = 0.029 |
| (1,2), (2,1)                             | 3   | 2/36 = 0.056 |
| (1,3), (2,2), (3,1)                      | 4   | 3/36 = 0.083 |
| (1,4), (2,3), (3,2), (4,1)               | 5   | 4/36 = 0.111 |
| (1,5), (2,4), (3,3), (4,2), (5,1)        | 6   | 5/36 = 0.139 |
| (1,6), (2,5), (3,4), (4,3), (5,2), (6,1) | 7   | 6/36 = 0.167 |
| (2,6), (3,5), (4,4), (5,3), (6,2)        | 8   | 5/36 = 0.139 |
| (3,6), (4,5), (5,4), (6,3)               | 9   | 4/36 = 0.111 |
| (4,6), (5,5), (6,4)                      | 10  | 3/36 = 0.083 |
| (5,6), (6,5)                             | 11  | 2/36 = 0.056 |
| (6,6)                                    | 12  | 1/36 = 0.029 |

: Probability distribution for the sum of two dice {#tbl-sum-two-dice}

(b) @fig-sum-two-dice is a bar graph in which the height of each bar is the probability of the corresponding value of the sum. The graph shows that the distribution is symmetric, with a mode at 7.

```{r sum-two-dice}
#| label: fig-sum-two-dice
#| out-width: 70%
#| fig-cap: | 
#|     Bar graph showing the probability distribution for the sum of two dice.

# Create data frame for the distribution
dice_data <- data.frame(
  Sum = 2:12,
  Probability = c(1,2,3,4,5,6,5,4,3,2,1) / 36
)

# Create bar plot
ggplot(dice_data, aes(x = factor(Sum), y = Probability)) +
  geom_bar(stat = "identity", 
           fill = IMSCOL["blue", "full"]) +
  labs(x = "Sum of the two dice",
       y = "Probability") 
```
:::

```{r}
#| include: false
terms_chp_4 <- c(terms_chp_4,
                 "distribution of discrete RV")
```
## Continuous random variables {#sec-continuous-rv}

Measurements of human characteristics such as height, weight, or systolic blood pressure are not limited to a finite number of discrete values. They can take on all values within a certain range. **Continuous random variables** \index{continuous random variable} are used to model or represent these types of measurements. For the discrete random variables discussed earlier, such as the sum of two dice, it was possible to specify a random phenomenon or process that gave rise to the distribution of a variable, such as in @tbl-sum-two-dice. That is rarely possible with continuous random variables; the phenomena that lead to variations in weight, for instance, are not well enough understood. Instead, the distributions of continuous variables are often chosen to match historical data.

```{r}
library(nhanesA)
library(dplyr)
library(ggplot2)

#  BMX_J is 2021 - 2023 post-pandemic cycle
bmx  <- nhanes("BMX_L")   # Body Measures
demo <- nhanes("DEMO_L")  # Demographics

bmx_demo <- inner_join(bmx, demo, by = "SEQN")

adult_wt_ht <- bmx_demo |> 
  filter(RIDAGEYR >= 18,
         !is.na(BMXWT),
         !is.na(BMXHT),
         !is.na(RIAGENDR),
         BMXWT > 0,
         BMXHT > 0) |> 
  select(BMXWT, BMXHT, RIAGENDR) 

```

```{r}
nw <- nrow(adult_wt_ht)
mu_w <-round(mean(adult_wt_ht$BMXWT), 2)
med_w <-round(median(adult_wt_ht$BMXWT), 2)

```

@fig-adult-wt-box-hist-prop shows the distribution of the weight in kilograms (kg) of `r nw` members of the US population. The data come from the National Health and Nutrition Examination Survey (NHANES) conducted between 2021 and 2023. Both the box plot and the histogram show that the distribution is skewed right with many outliers; the mean weight (`r mu_w`) is larger than the median (`r med_w`). The graph shows that it would be inappropriate to think of weights as symmetrically distributed.  

The minimum and maximum observed weights are `r min(adult_wt_ht$BMXWT)`kg and `r max(adult_wt_ht$BMXWT)`kg. ^[The maximum weight in the data is certainly large, but human weights have been recorded as large as 500 - 600kg.]  The histogram shows relative frequencies, so the heights of the bars represent the fraction of the observations found in each interval of width 5kg. For instance, approximately 3.8%, or 0.038, of the weights are between 50 and 55kg, and another 6.3% are between 55 and 60kg, so approximately 10.1% are between 50 and 60kg. 

Both plots illustrate that the middle 50% of weights of US adults is approximately symmetric but overall the distribution skews toward being overweight or obese, a tendency that has been widely studied in the US population.  What random variable $W$ might be used to model this distribution?


```{r}
#| label: fig-adult-wt-box-hist-prop
#| out-width: 80%
#| fig-cap: | 
#|     Distribution of weights (kg) US adults 18 or older.


library(patchwork)

p1 <- ggplot(adult_wt_ht, aes(x = BMXWT)) +
  geom_boxplot() +
  scale_color_openintro("two") +
  labs(y = NULL, x = NULL) +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.ticks.y = element_blank(),
    axis.text.x = element_blank(),
    axis.title.x = element_blank(),
    axis.ticks.x = element_blank()
   # plot.margin = margin(b = 0)
  ) 
#  scale_x_continuous(breaks = seq(0, 220, 50)) 

p2 <- ggplot(adult_wt_ht, aes(x = BMXWT)) +
  geom_histogram(aes(y = after_stat(count / sum(count))),
         breaks = seq(0, 225, 5),
         closed = "right") +
     labs(x = "Weight (kg)", y = "Proportion") +
     scale_x_continuous(
    breaks = seq(0, 220, 50))+
    scale_y_continuous(breaks = seq(0, 0.10, 0.0125))

p1/p2 + plot_layout(heights = c(0.25, 0.75))

```

The histogram in @fig-adult-wt-hist-density uses the same data as @fig-adult-wt-box-hist-prop but is drawn on a \index{density scale} **density scale**. The vertical axis has been re-scaled so that the height of the bars is now the proportion of observations divided by the width of the bins (5kg in this case). When a histogram is drawn on the density scale, the proportion of observations falling in a bin is now the area of the bar, since multiplying the density value by the bin width recovers the proportion. For instance, the probability that a weight is between 50 and 60kg can be read directly from the plot as a little larger than 0.010. Since proportions like probabilities add to 1, the total area under the histogram drawn on a density scale will be 1.

### Density functions for continuous random variables {#sec-density-functions}

A random variable $W$ used to model weights should reflect the shape of the histogram, but "smooth out" the features of the data from the sample of `r nw` observations.  The smooth red curve in the plot is the best fitting smooth **density curve** \index{density curve} to the histogram where the total area under the curve is 1. The extended right tail in the curve reflects the large weights that occur in small numbers. Since the curve approximates the histogram, the probability of an observation in a range of heights will be approximately the area under the density curve in that range of heights.

```{r}
#| label: fig-adult-wt-hist-density
#| out-width: 70%
#| fig-cap: | 
#|     Histogram of the weights (kg) adults 18 years or older from NHANES, density scale.

ggplot(adult_wt_ht, aes(x = BMXWT)) +
  geom_histogram(aes(y = after_stat(density)),
                 breaks = seq(0, 225, 5),
                 closed = "right") +
     labs(x = "Weight (kg)", y = "Density") +
     scale_x_continuous(
    breaks = seq(0, 220, 50)) +
   geom_density(
    color = "red",
    linewidth = 1.0,
    alpha = 0.8,
    bw = 5.5
  ) 

```

If the NHANES data are approximately representative of the US adult population, the density function shows the distribution of a continuous  random variable $W$ representing the weight of a randomly sampled individual from the US population.  Unlike a discrete random variable, where distributions can be displayed in a table, the distribution of a continuous random variable is summarized in a **probability density function** \index{probability density function}.

::: {.important data-latex=""}
**Probability density function.**

The probability density function $f(x)$ of a continuous random variable $X$ is a curve that satisfies:

-   The values of $f(x)$ are non-negative.

-   The total area under the curve $f(x)$ is 1.^[Using calculus notation, $\int f(x)dx = 1$]

-   The probability that $X$ has a value between two numbers $a$ and $b$, denoted by $P(a \leq X \leq b)$, is the area under $f(x)$ between $x = a$ and $x = b$.
:::

The probability density function in @fig-adult-wt-density-fit is a theoretical density function for a random variable $W$ that can be used to model the distribution of observed weights from randomly selected members of the US adult population. The shaded area under the curve between 50 and 60kg is 0.102, close to the earlier values estimated from the histogram drawn on the proportion scale. Software is generally used to calculate area under a density curve. For the purposes of this text, however, the concept of a density function is more important than the details of how to calculate probabilities. It is important to remember that areas are used to calculate probabilities with a density function, not the height of the curve.

```{r}
#| label: fig-adult-wt-density-fit
#| out-width: 70%
#| fig-cap: | 
#|     Smooth density function matching the distribution of weights.  Area of the shaded region is the probability of observing a weight between 50 and 60 kg.
p <- ggplot(adult_wt_ht, aes(BMXWT)) +
        geom_density(bw = 5.5)
pb <- ggplot_build(p)
curve <- pb$data[[1]][, c("x", "y")]  # density grid used by ggplot

a <- 50; b <- 60
dx <- diff(curve$x)[1]
area_ab <- sum(curve$y[curve$x >= a & curve$x <= b]) * dx

ggplot(curve, aes(x, y)) +
  geom_line(linewidth = 1.0) +
  geom_area(
    data = subset(curve, x >= a & x <= b),
    fill = IMSCOL["blue", "full"], alpha = 0.75
  ) +
  labs(
    x = "Weight (kg)", y = "Density") +
    scale_x_continuous(
    breaks = seq(0, 220, 50))

```

::: {.workedexample data-latex=""}
What is the probability that a randomly selected person weighs **exactly** 53.47 kg? Assume that weight can be measured perfectly.

------------------------------------------------------------------------

This probability is zero. A person's weight might be close to 53.47kg, but not exactly that value. This is consistent with the definition of probability as an area under the density curve; there is no area captured above a single point.
:::

```{r}
#| include: false
terms_chp_4 <- c(terms_chp_4,
                 "continuous RV",
                 "density scale",
                 "density function",
                 "area under a density function")
```

## Expected value of a random variable {#sec-expected-value-rv}

Just like distributions of data, distributions of random variables have means, variances, and standard deviations; these characteristics are computed a bit differently for random variables but have the same interpretation. The mean of a random variable is called its **expected value** \index{expected value} and written $E(X)$. It is computed differently for discrete and continuous random variables but it has the same interpretation in both cases.

### Expected value of a discrete random variable {sec-expected-value-discrete-rv}

::: {.important data-latex=""}
If $X$ is a discrete variable taking on the $k$ values $x_1 \ldots x_k$ with probabilities $P(X=x_1) \ldots P(X=x_k)$, the expected value of $X$ is the sum of each value multiplied by its corresponding probability:

$$
\begin{aligned}
E(X)  &= x_1 P(X=x_1) + \cdots + x_k P(X=x_k) \\
      &= \sum_{i=1}^{k} x_i P(X=x_i).
\end{aligned}
$$ {#eq-expectation-discrete-rv}

The Greek letter $\mu$\index{Greek!mu ($\mu$)} may be used in place of the notation $E(X)$. 
:::


::: {.workedexample data-latex=""}

Calculate the expected value of $X$, where $X$ represents the number of heads in three tosses of a fair coin.

------------------------------------------------------------------------

$X$ can take on the four values 0, 1, 2, and 3. The probability of each value $x_k$ is given in @tbl-dist-coin-tossing.

\begin{align*}
E(X) &= x_1 P(X = x_1) + \dots + x_k P(X = x_k)\\
&= (0)(P(X=0)) + (1)(P(X=1)) + (2)(P(X=2)) + (3)(P(X = 3)) \\
&= (0)(1/8) + (1)(3/8) + (2)(3/8) + (3)(1/8) = 12/8 \\
&= 1.5.
\end{align*}

The expected value of $X$ is 1.5.
:::

The expected value for a random variable represents the average outcome. For example, $E(X)=1.5$ represents the average number of heads in three tosses of a coin, if the three tosses were repeated many times. With discrete random variables the expected value need not be one of the possible outcomes of the variable, as in this example.

::: {.guidedpractice data-latex=""}
Calculate the expected value of $Y$, where $Y$ represents the number of heads in three tosses of an unfair coin, where the probability of heads is 0.70. [^random-variables-3]
:::

[^random-variables-3]: First, calculate the probability distribution of $Y$. $P(Y=0) = (1 - 0.70)^3 = 0.027$ and $P(Y=3) = (0.70)^3 = 0.343.$ Note that there are three ways to obtain 1 head (HTT, THT, TTH), thus, $P(Y=1) = (3)(0.70)(1 - 0.70)^2 = 0.189$. By the same logic, $P(Y = 2) = (3)(0.70)^2( 1- 0.70) = 0.441$. Thus, $E(Y) = (0)(0.027) + (1)(0.189) + (2)(0.441) + (3)(0.343) = 2.1$. The expected value of $Y$ is 2.1.

```{r}
#| include: false
terms_chp_4 <- c(terms_chp_4,
                 "expected value"
                 )
```

### Expected value of a continuous random variable {#sec-expected-value-continuous-rv}


```{r mean-sd-adult-weights}

dens <- density(adult_wt_ht$BMXWT)
df <- data.frame(x = dens$x, y = dens$y)

# Mean of the density estimate
mean_kde <- round(sum(df$x * df$y) / sum(df$y), 1)

# Variance of the density estimate
var_kde <- round(sum((df$x - mean_kde)^2 * df$y) / sum(df$y), 2)
sd_kde <- round(sqrt(var_kde), 2)


```

Expected values of continuous random variables are calculated using either calculus or software, so there are no formulas like @eq-expectation-discrete-rv. But the expected value of a continuous random variable does have a concrete interpretation. Think of the area under a density curve as a distribution of weight or mass, with larger values of the density indicating more mass. The expected value $\mu$ of a continuous random variable is the point on the horizontal axis where the density would balance. @fig-chi-sq-density-with-mean illustrates that concept.[^random-variables-4]

[^random-variables-4]: This is figure 3.21 in [OpenIntro Statistics, 4^th^ ed.](https://www.openintro.org/book/os/)

When the density is skewed right with a heavy tail, the balance point moves right to maintain the balance.

```{r}
#| label: fig-chi-sq-density-with-mean
#| out-width: 70%
#| fig-cap: | 
#|     A theoretical probability density function with mean $\mu$.  If the density represents a distribution of mass, it will balance at the value $\mu$ on the horizontal axis.
x <- seq(0, 22, 0.01)
y <- dchisq(x, 5)
M <- weighted.mean(x, y)

par(mar = c(1.65, 0, 0, 0), mgp = c(5, 0.5, 0))
plot(x, y + 0.035,
     type = 'l',
     ylim = range(c(0.025, y + 0.035)),
     axes = FALSE)
axis(1, at = c(-100, M, 100), labels = c('', expression(mu), ''))
lines(c(0, 22), rep(0.035, 2))
polygon(x, y + 0.035, col = IMSCOL["blue", "full"])
polygon(c(M - 20, M + 20, M),
        c(-0.2, -0.2, 0.035),
        col = COL[4])


```

@fig-weight-density-with-mean illustrates the idea with the density function of $W$ used to model the distribution of weights NHANES. Values on the x-axis are not shown, but the balance point is $E(W)$ = `r mean_kde`kg.

```{r}
#| label: fig-weight-density-with-mean
#| out-width: 80%
#| fig-cap: | 
#|     The probability density function for the weights $W$ with mean $\mu_W$ = 82.9 kg. 
M <- mu_w
tri_df <- data.frame(
  x = c(M - 12, M + 12, M),
  y = c(-0.002, -0.002, 0.0)   # base well below, tip touches baseline
)
ggplot(adult_wt_ht, aes(BMXWT)) +
    geom_density(bw = 5.5,
                    fill = IMSCOL["blue", "full"],
                    alpha = 1.0,
                    color = "black") +
    annotate(
             "segment",
              x = 0, xend = 250, 
              y = -0.002, yend = -0.002,
              color = "black", 
             linewidth = 0.4) +
   annotate(
           "segment",
           x = M, xend = M,
           y = - 0.003, yend = -0.002,   # short vertical line
           color = "black", linewidth = 0.4
  ) +
    geom_polygon(
                data = tri_df,
                aes(x = x, y = y),
                fill = IMSCOL["red", "full"],
                inherit.aes = FALSE) +
    scale_x_continuous(limits = c(0, 250),
                     breaks = M,
                     labels = c(expression(mu))) +
    labs(x = NULL, y = NULL) +  
    theme(
    axis.title = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid = element_blank()
  )
        
```



## Variance and standard deviation of a random variable {#sec-variance-rv}

The variability of a random variable can be described by its \index{variance} **variance** and \index{standard deviation} **standard deviation**. For a dataset with $n$ observations, the variance is computed by squaring deviations from the mean of the data, $(x_i - \bar{x})^2,$  then computing the average of those squared deviations using a denominator of $n - 1$. (See @sec-summarizing-numerical-data.) 

In the case of a random variable, the squared deviations from the mean $\mu$ are used instead of from the sample mean $\bar{x}$, and the average is a weighted average using the corresponding probabilities. This weighted sum of squared deviations defines the variance of the random variable; the standard deviation is the square root of the variance.

::: {.important data-latex=""}

### Variance of a discrete random variable {#sec-variance-discrete-rv}

If $X$ takes on the $k$ outcomes $x_1$, ..., $x_k$ with probabilities $P(X=x_1)$, \dots, $P(X=x_k)$ and has expected value $\mu=E(X)$, then the variance of $X$, denoted by $\text{Var}(X)$ or $\sigma^2$, is
$$
\begin{aligned}
\text{Var}(X) &= (x_1-\mu)^2 P(X=x_1) + \cdots \notag + (x_k-\mu)^2 P(X=x_k) \notag \\
&= \sum_{i=1}^{k} (x_i - \mu)^2 P(X=x_i).
\end{aligned}
$${#eq-variance-discrete-rv}

The standard deviation of $X$, labeled $\text{SD}(X)$ or $\sigma$ \index{Greek!sigma ($\sigma$)}, is the square root of the variance. The standard deviation is sometimes written as $\sigma_X$ to eliminate ambiguity.  The standard deviation has the same units of measurements as the original observations.

:::
 
The variance of a random variable can be interpreted as the expectation of the terms $(x_i - \mu)^2$; i.e., $\sigma^2 = E(X - \mu)^2$. This compact form can be helpful for understanding the concept of variability of a random variable; variance is simply the average of the squared deviations from the mean, just as it is when calculating the variance of observations in a dataset.

Algebra and the properties of expectation can be used to show that the variance can also be written
$$
\sigma^2 = E(X^2) - [E(X)]^2 = \mu_{X^2} - (\mu_X)^2.
$$ {#eq-variance-alt}

::: {.workedexample data-latex=""}

Compute the variance and standard deviation of $X$, the number of heads in three tosses of a fair coin, using both @eq-variance-discrete-rv and @eq-variance-alt.

------------------------------------------------------------------------

Using @eq-variance-discrete-rv, $k = 4$ and $\mu_X = E(X) = 1.5$. 
\begin{align*}
	\sigma_X^2 &= (x_1-\mu_X)^2P(X=x_1) + \cdots + (x_4-\mu)^2 P(X=x_4)  \\
	&= (0- 1.5)^2(1/8) + (1 - 1.5)^2 (3/8) + 
	(2 -1.5)^2 (3/8) + (3-1.5)^2 (1/8) \\
	&= 3/4.
\end{align*}

Using @eq-variance-alt
\begin{align*}
  E(X^2) &= 0^2(1/8) + 1^2 (3/8) + 2^2 (3/8) + 3^2 (1/8) \\
         &= 3,
\end{align*}
so the variance is $3 - (3/2)^2 = 3/4$.

Since the variance is $3/4 = 0.75$, the standard deviation is $\sigma = \sqrt{3/4} = 0.866$. 
:::

The coin tossing scenario provides a simple illustration of the mean and variance of a random variable.  A more realistic example is discussed in the next section -  calculating expected health care costs.


```{r}
#| include: false
terms_chp_4 <- c(terms_chp_4, 
               c("variance of an RV",
                 "standard deviation of an RV")
)
```

### The variance of a continuous random variable {#sec-variance-continuous-rv}


The variance of a continuous random variable has the same interpretation as that of a discrete variable -- it is the average of the squared deviation of each value from the mean.

::: {.important data-latex=""}
**Variance of a continuous random variable.**

Suppose $X$ is a continuous random variable with mean $\mu = E(X)$.  The variance of $X$ is ^[The calculus based definition is  $\text{Var}(X) = \int (x - \mu)^2 f(x)dx$, but that is not needed in this text and can be ignored.]
$$
 \text{Var}(X) = E(X - \mu)^2,
$$
and its standard deviation is 
$$
  \sigma_X = \sqrt{\text{Var}(X)}.
$$
:::


The calculations are not shown here, but the variance and standard deviation of adult weights in the NHANES data are `r var_kde`kg^2^ and `r sd_kde`kg, respectively.  


## Sums of independent random variables {#sec-sum-indep-rvs}

Sums of random variables arise naturally in many problems. When rolling two dice, the total $X$ shown on the roll is the sum of the two faces showing, $X_1 + X_2$.  If $Y$ is the total time commuting to work during a 5-day work week, $Y = Y_1 + Y_2 + \cdots +Y_5$, where the individual $Y$ values represent the commuting time for each of the 5 days.

Sums of random variables represent a special case of linear combinations of variables.  

::: {.important data-latex=""}
**The expected value of a sum of random variables.**

Suppose $X$ and $Y$ are random variables.  Then 

$$
E(aX + bY) = aE(X) + bE(Y) = a\mu_X + b\mu_Y.
$$ {#eq-mean-linear-combination-rv}

:::

The formula easily generalizes to a sum of any number of random variables.  The expected time commuting during the week will be 
$$
  E(Y) = E(Y_1) + E(Y_2) + \cdots + E(Y_5).
$$

The formula implies that for a random variable $Z$, $E(a + Z) = a + E(Z)$.  

A similar formula applies to the variance of a sum of random variables, but with an important additional condition, independence.   Informally two random variables are **independent** \index{independent random variables} if the outcome of one is not associated with outcomes of the other. When members of a large population are sampled randomly to participate in a survey, the survey responses are independent.  When study participants are assigned randomly to an intervention versus a control treatment, study outcomes are independent.

When two variables are not independent, they are not surprisingly called dependent.  Dependence is encountered more often than independence. People with larger shoe size tend to be taller than people with smaller feet, so shoe size and height are not independent.  People with lower income tend to rent rather than own a home, so income and home ownership are not independent.   

The concepts of independent random variables and correlated data are closely related. The numerical observations of independent random variables will be uncorrelated, while dependent random variables lead to correlated observations.

The mathematical definition for independent random variables uses the definition of independent events, but is not needed here.  For the purposes of this text it is sufficient to use the informal definition and insight into how variables are measured.

::: {.guidedpractice data-latex=""}

Which of the following pairs of measurements can be represented by independent random variables?  (a) Blood pressure and weight. (b) The response to a medication of two individuals who are not related and do not know each other. (c) Eye color and choosing to respond to an online survey. (d) Height of two siblings.  (e) Family income and zip code of residence. (f) Age and work status. [^random-variables-501]
:::

[^random-variables-501]: b. and c. Eye color and survey participation are independent since there is no biological evidence of association.  The others are dependent.



::: {.important data-latex=""}
**The variance of a sum of random variables.**

$$
\text{Var}(aX + bY) = a^2 \text{Var}(X) + b^2\text{Var}(Y).
$$ {#eq-variance-linear-combination-ind-rv}
This equation is valid only if the random variables are independent of each other. 

:::

For the sum $a + bZ$, the variance is $b^{2} \text{Var}(Z)$, since a constant $a$ has variance 0. When $b = 1$, variance of $a + Z$ is $\text{Var}(Z)$ - adding a constant to a random variable has no effect on its variability.

::: {.workedexample data-latex=""}

The average body temperature in a healthy population measured in the Fahrenheit scale has been shown in some studies to be 97.9F, with a standard deviation of approximately 0.6F.  What are the average and standard deviation of body temperatures measured in the Celsius scale? 

------------------------------------------------------------------------

If $F$ and $C$ are random variables representing body temperature in Fahrenheit and Celsius scales, the conversion from $F$ to $C$ is
$$
  C = \frac{5}{9}F - 17.8
$$
Average temperature in Celsius is
$$
  E(C) = \frac{5}{9} E(F) - 17.8 = 0.556(97.9) - 17.8 = 36.6\text{ degrees Celsius}
$$
The variance on the Celsius scale is
\begin{align*}
  \text{Var}(C) &= (\frac{5}{9})^2 \text{Var}(F) \\
                &= (0.309)(0.6)^2 \\
                &= 0.111
\end{align*}
The standard deviation on the Celsius scale is $\sqrt{0.111}$ = `r sqrt(0.111)`.

:::

Equations [-@eq-mean-linear-combination-rv] and [-@eq-variance-linear-combination-ind-rv] are illustrated in more detail in the next section.

```{r}
#| include: false
terms_chp_4 <- c(terms_chp_4, 
               c("expected value of sum of RVs",
                 "variance of sum of independent RVs",
                 "standard deviation of sum of independent RVs")
)
```


## Example: the cost of health insurance {#sec-cost-of-hmo-insurance}

```{r hmo-visits-costs}

v_values <- c(0, 1, 2, 3, 4)
v_probs <- c(0.44, 0.19, 0.06, 0.06, 0.25)
s <- sum(v_probs)
v_mean <- sum(v_values * v_probs)
v_var <- sum((v_values - v_mean)^2 * v_probs)
v_sd <- sqrt(v_var)
c_values <- c(1200, 1220, 1240, 1260, 1280)
c_probs <- v_probs
c_mean <- 1200 + 20 * v_mean
c_var <- 3 * (20)^2 * v_var
c_sd <- sqrt(c_var)
```


In most  health insurance plans in the United States, members pay annually in three categories: a monthly premium, a deductible amount paid  each year before the insurance covers prescription drugs or hospitalization, and the ``out-of-pocket'' co-payments for each outpatient visit.   This example illustrates a hypothetical setting in which a Health Maintenance Organization uses its records to communicate the expected cost of the premium and outpatient visits to potential plan members who generally need only routine care.   It does not consider the cost of prescription drugs, specialized diagnostic tests or major events like hospitalization, and so does not include the deductible.

Since health care visits and expenses increase with age, the HMO decides to calculate this average among members age 25 - 40 who have not been diagnosed with a serious chronic disease, like diabetes or chronic heart disease.  After reviewing its records for the last several years, the HMO found that the majority of plan members in this segment had either no or one visit in a year, and none had more than 4 visits.  The plan analytics department used the data to create a probability distribution for annual visits of members in this population.

Suppose $V$ denotes the number of annual visits for a randomly chosen member of the plan age 25 - 40 with no serious chronic diseases.  @tbl-hmo-visit-dist shows the distribution of the number of annual visits, with $v_i$ representing the values of the random variable $V$.  

| $i$                     |   1    |    2    |    3    |    4    |   5    |
|:-----------------------:|:------:|:-------:|:-------:|:-------:|:------:|
| Number of visits, $v_i$ |   0    |    1   |    2    |    3    |   4     |
| $P(V = v_i)$            |`r v_probs[1]`|`r v_probs[2]`|`r v_probs[3]`|`r v_probs[4]`|`r v_probs[5]`|

: Distribution of HMO office visits. {#tbl-hmo-visit-dist}


::: {.guidedpractice data-latex=""}

Show that @tbl-hmo-visit-dist satisfies the conditions for a  probability distribution and describe the distribution in words to someone who has not studied probability. 
[^distributions-601]
:::

[^distributions-601]:@tbl-hmo-visit-dist satisfies the conditions of a discrete probability distribution: all probabilities are between 0 and 1, and the values sum to 1. `r v_probs[1] *100`% of this population never have a visit for outpatient care, while `r (v_probs[1] + v_probs[2]) * 100`% have 1 or fewer visits.  The least likely number of visits are 3 or 4 per year (`r v_probs[3]*100`% in each category), while a somewhat surprising number (`r v_probs[5]*100`% of members) have 4 visits per year. In this population plan members are either very healthy and never have an outpatient visit, or have a minor condition, such as allergy shots or physical therapy that causes them to book outpatient visits once a quarter. 

::: {.workedexample data-latex=""}

Construct a bar plot of the distribution of annual visits.  Describe the shape of the distribution using the terminology in @sec-intro-to-data used to describe distributions of data.

------------------------------------------------------------------------

@fig-bar-plot-hmo-visit-dist is a bar plot of this distribution.  The distribution is skewed right and has two prominent peaks at $V = 0$ and $V=4$, so is bimodal.  

```{r bar-plot-hmo-visit-dist}
#| label: fig-bar-plot-hmo-visit-dist
#| fig-cap: Bar plot of the distribution of the number of visits per year in a hypothetical HMO among healthy adults age 25 - 40.
#| fig-alt: |
#|   coming 
#| out-width: 70%

# Create the data frame
v_df <- tibble(v_values, v_probs)
ggplot(v_df, aes(x = factor(v_values), y = v_probs)) +
  geom_col(fill = IMSCOL["blue", "full"]) +
  scale_y_continuous(limits = c(0, 0.5), expand = c(0, 0)) +
  labs(
    x = "Annual visits",
    y = "Probability"
  ) 
```

:::

::: {.workedexample data-latex=""}

Calculate the mean and standard deviation for the number of visits.  Provide an interpretation of the mean. Is it a good summary measure for potential plan members?

------------------------------------------------------------------------

Using @eq-expectation-discrete-rv, the expected value of the annual number of visits is 
\begin{align*}
 E(V) &= (0)(0.44) + (1)(0.19) + \cdots + (4)(0.20)   \\
  &= `r round(v_mean,2)` \text{ visits per year}.
\end{align*}
Using @eq-variance-discrete-rv, the variance of $V$ is 
\begin{align*}
 \text{var}(V) &= (0 - `r v_mean`)^2 (0.44) + (1 - `r v_mean`)^2(0.19) + \cdots + (4 - `r v_mean`)^2 (0.25) \\
 &= `r round(v_var, 2)`.
\end{align*}
The standard deviation is $\sigma = \sqrt{\text{Var}(V)} = `r round(v_sd,2)`$ visits per year.

Since $E(V) = 1.49$, this population of plan members has (on average) approximately 1.5 outpatient visits per year.  From the shape of the distribution the mean is not a good summary measure for the distribution. More than half of the members (`r (v_probs[1] + v_probs[2]) * 100`%) have either 0 or 4 visits, and 1.5 is close to one of the least likely number of visits.
:::

::: {.workedexample data-latex=""}

Suppose now the monthly premium for the plan is \$100 and the co-payment for each office visit and standard services like physical therapy is \$20. (a) What can a new plan member matching the characteristics of this population expect to pay in the coming year? (b) If the annual premium and co-payment do not change for the next 3 years, what would be their expected  costs during that period?

------------------------------------------------------------------------

a. The total annual cost is a function of the premium and the co-payment.  If $C$ denotes annual outpatient costs,
$$
 C = (12)(100) + (20)(V).
$$
Using @eq-mean-linear-combination-rv,
\begin{align*}
 E(C) &= E(1200 + (20)(V)) \\
      &= 1200 + (20)(E(V)) \\
      &= 1200 + (20)(`r v_mean`) \\
      &= `r round(1200 + 20 * v_mean, 2)` \text{ dollars }
\end{align*}
It would be statistically correct for the HMO to advertise that  a plan member in this population can expect to pay approximately \$`r round(1200 + 20 * v_mean, 0)` annually but it would be misleading because of the shape of the distribution.  Forty-four percent of patients pay only the premium (\$1200), while another 25% pay the cost of the premium + 4 visits (\$1280). The  plan should also remind members that these costs do not include the deductible for prescription drugs.

b. If $C_1, C_2$ and $C_3$ represent the cost of care for each of the three years,
\begin{align*}
E(C_1 + C_2 + C_3) &= E(C_1) + E(C_2) + E(C_3) \\
      &= (3)(E(C)) \\
      &= (3)(`r round(1200 + 20 * v_mean, 2)`) \\
      &= `r round(3 * (1200 + 20 * v_mean),2)` \text{ dollars}
\end{align*}

:::


::: {.workedexample data-latex=""}

Using the same assumptions of the above example, calculate the standard deviation of 3 years of costs for outpatient visits. Does it seem as though the standard deviation is a reasonable measure of spread for this distribution?

------------------------------------------------------------------------

The variance for a single year of costs is $\text{Var}(C)$.  Using @eq-variance-linear-combination-ind-rv,
\begin{align*}
  \text{Var}(C) &= \text{Var}(1200 + (20)(V)) \\
                &= (20^2) \text{Var}(V) \\
                &= (400)(`r round(v_var, 2)`) \\
                &= `r round(c_var, 2)` \text{ dollars}^2.
  \end{align*}

Under the assumption that costs of each year are independent of those in previous years (a very strong assumption!), the variance for three years will be 

\begin{align*}
\text{Var}(C_1 + C_2 + C_3) &= \text{Var}(C_1) + \text{Var}(C_2) +\text{Var}(C_3) \\
                            &= (3)\text{Var(C)} \\
                            &= (3)(`r round(c_var,2)`) \\
                            &= `r 3 * round(c_var,2)` \text{ dollars }^2.
\end{align*}
The standard deviation is $\sqrt{`r 3* round(c_var,2)`} = \$`r round(sqrt(3* c_var), 2)`$.  It is not a good measure of spread, however, because of the bimodal nature of the distribution.

:::

::: {.guidedpractice data-latex=""}

Why is the assumption that the variances for the three years can be added such a strong (and perhaps untenable) assumption?
[^distributions-602]
:::

[^distributions-602]: The health characteristics of members of this population may not change from year to year, but they are unlikely to be independent: someone who has not visited the HMO for an outpatient visit is more likely not to visit in the next year, for instance, so the conditional probability of no visits in a coming year given that there were none in the previous year may be  different (and probably larger) than the unconditional probability of no visits.

The calculations for the mean and standard deviation of the annual cost of care did not use directly the distribution for annual cost, but instead used formulas for sums of variables.  The probability distribution for the annual cost of the premium plus co-pay for outpatient visits can be derived using the distribution of number of annual visits.  The expense to the plan member will be the \$1200 premium with probability 0.40 (the probability of no outpatient visits) when there are no visits, \$1220 with probability 0.10 when there is one visit, etc.  @fig-bar-plot-hmo-cost-dist is a bar plot for the distribution.

```{r bar-plot-hmo-cost-dist}
#| label: fig-bar-plot-hmo-cost-dist
#| fig-cap: Bar plot of the distribution of the annual costs per year in a hypothetical HMO among healthy adults age 25 - 40.
#| fig-alt: |
#|   coming 
#| out-width: 70%

# Create the data frame
v.df <- data.frame(
  v.values = c(1200, 1220, 1240, 1260, 1280),
  v.probs = c(0.40, 0.10, 0.05, 0.05, 0.40)
)

ggplot(v.df, aes(x = factor(v.values), y = v.probs)) +
  geom_col(fill = IMSCOL["blue", "full"]) +
  scale_y_continuous(limits = c(0, 0.5), expand = c(0, 0)) +
  labs(
    x = "Annual cost",
    y = "Probability"
  ) 
```


Because of the relationship between cost and the number of visits, Figures  [-@fig-bar-plot-hmo-visit-dist] and [-@fig-bar-plot-hmo-cost-dist] are similar: the bar heights showing probabilities are identical, and the number of visits have been replaced by the cost associated with each number of visits.  The distribution has the same right-skewed,  bimodal shape in both cases.

::: {.workedexample data-latex=""}

The median $m$ of a discrete random variable $X$ is the value that satisfies 
$$
P(X \leq m) \;\geq\; 0.5
\quad \text{and} \quad
P(X \geq m) \;\geq\; 0.5.
$$
Find the median annual cost for the premium and outpatient visits.  How does it compare to the expected cost?  

------------------------------------------------------------------------

The median is computed by finding the smallest value $m$ such that $P(X \leq m) \;\geq\; 0.5$.  For the random variable $C$ representing annual cost, that value is \$1220 since 
$$
P(C \leq 1200) = 0.40 < 0.50$$
while 
$$P(C \leq 1220) = 0.40 + 0.10 = 0.50 \geq 0.50
$$.

From the earlier example, the mean cost is $E(C)=$ \$1239.  Just as with distributions of data, the mean is larger than the median in this right-skewed distribution.  

:::

## Distributions for pairs of random variables {#sec-pairs-rv}

### Joint, marginal and conditional distributions {#sec-joint-dist-rv}

The concepts of joint, marginal and conditional probabilities were introduced in @sec-conditional-prob-gss. Analogous concepts exist for random variables.  

Suppose the analytics team of a health plan is studying the relationship between the number of primary care visits ($X$) and filled prescriptions ($Y$) of a plan member in a  year, based on a large sample of plan members from medical records. For simplicity, assume that plan members have between 0 and 2 visits each years, and between 0 and 2 total prescriptions filled in a year. Some visits will result in no prescriptions, and some in more than 1.  @tbl-joint-dist-visits-drugs shows the probabilities for all 9 combinations of $X$ and $Y$.  For instance, the probability that a plan member will have 1 visit and 2 prescriptions is 0.05.  The collection of these 9 probabilities is the joint distribution of $(X, Y)$.

| $X \backslash Y$ | 0    | 1    | 2    | 
|:---------------:|:----:|:----:|:----:|
| 0 | 0.20 | 0.08 | 0.02 | 0.30 |
| 1 | 0.07 | 0.18 | 0.05 | 0.30 |
| 2 | 0.03 | 0.10 | 0.27 | 0.40 |

: Joint distribution of clinic visits ($X$) and prescription refills ($Y$). {#tbl-joint-dist-visits-drugs}

::: {.important data-latex=""}
**Joint distribution.**

The \index{joint distribution} **joint distribution** $p_{X, Y}(x, y)$ for a pair of discrete or categorical random variables $(X, Y)$ is the collection of probabilities
\begin{align*}
p_{X,Y}(x_i,y_j) = P(X=x_i \text{ and } Y = y_j)
\end{align*}
for all pairs of values $(x_i,y_j)$ that the random variables $X$ and $Y$ take on. ^[There is a similar definition for pairs of continuous random variables based on the concept of a joint density function that is not explored here.]

When there is no ambiguity the subscript $X, Y$ may not included in the notation for $p(x_i, y_j)$.

:::

Using @tbl-joint-dist-visits-drugs, $p(1, 2) = 0.05$.

Joint distributions are often displayed in tabular form as in @tbl-joint-dist-visits-drugs.  If $X$ and $Y$ have $k_1$ and $k_2$ possible values respectively, there will be $k_1 \times k_2$ possible $(x,y)$ pairs. This is unlike pairs of values $(x,y)$ observed in a dataset, where each observed value of $x$ is usually paired with only one value of $y$.  @tbl-gen-joint-dist  shows the general form of the table for the joint distribution when   $X$ and $Y$ are either discrete or categorical variables.


| **Values of $X$** | $y_1$ | $y_2$ | $\cdots$ | $y_{k_2}$ |
|:----------------:|:-----:|:-----:|:--------:|:---------:|
| $x_1$ | $p(x_1, y_1)$ | $p(x_1, y_2)$ | $\cdots$ | $p(x_1, y_{k_2})$ |
| $x_2$ | $p(x_2, y_1)$ | $p(x_2, y_2)$ | $\cdots$ | $p(x_2, y_{k_2})$ |
| $\vdots$ | $\cdots$ | $\cdots$ | $\cdots$ | $\cdots$ |
| $x_{k_1}$ | $p(x_{k_1}, y_1)$ | $p(x_{k_1}, y_2)$ | $\cdots$ | $p(x_{k_1}, y_{k_2})$ |

: Table for a joint distribution.  Entries are probabilities for pairs $(x_i, y_j)$.  {#tbl-gen-joint-dist}

@tbl-gss-age-health-prop in @sec-gss-example was a table of joint probabilities for age and health rating; it can also be viewed as the table of the joint distribution of the random variables $X$ = age of participant and $Y$ = health rating for an individual sampled from the population.

When two variables $X$ and $Y$ have a joint distribution, the \index{marginal distribution} **marginal distribution** of $X$ is the collection of probabilities for $X$ when $Y$ is ignored. ^[The marginal distribution of $X$ can be written as $p_X(x)$, and a specific value in the marginal distribution written as $p_X(x_i)$.]  If $X$ number of primary care visits and $Y$ number prescriptions, the event $(Y = 1)$ consists of the three  disjoint events $(X = 0 \text{ and } Y = 1)$, $(X = 1 \text { and } Y = 1)$ and $(X = 2 \text{ and }  Y = 1)$, so $P(Y = 1) = 0.08 + 0.18 + 0.10 = 0.30$, the sum of the second column of @tbl-joint-dist-visits-drugs. The column sums give the marginal distribution of $Y$, while the row sums are the values of the marginal distributions of $X$. 

The marginal distributions of number of visits and prescriptions filled  of $X$ and $Y$ are shown in the last row and column in @tbl-marg-dist-visits-drugs along with the earlier joint distribution. The term marginal distribution is apt in this setting---the marginal probabilities appear in the table margins.  


| $X \backslash Y$ | 0    | 1    | 2    | $P(X=x)$ |
|:---------------:|:----:|:----:|:----:|:--------:|
| 0 | 0.20 | 0.08 | 0.02 | 0.30 |
| 1 | 0.07 | 0.18 | 0.05 | 0.30 |
| 2 | 0.03 | 0.10 | 0.27 | 0.40 |
| $P(Y=y)$ | 0.30 | 0.36 | 0.34 | 1.00 |

: Joint and marginal distribution of clinic visits ($X$) and prescription refills ($Y$). {#tbl-marg-dist-visits-drugs}

For a pair of random variables $X$ and $Y$, the \index{conditional distribution} **conditional distribution** of $Y$ given a value $x$ of the variable $X$ is the probability distribution of $Y$ when its values are restricted to the value $x$ for $X$. Just as marginal and joint probabilities are used to calculate conditional probabilities, joint and marginal distributions can be used to obtain conditional distributions. 

\begin{onebox}{Conditional distribution}

Suppose in a pair of random variables $(X, Y)$ the value of $X$ is known to be $x$. The conditional distribution $p_{Y|X(y|x)$ is the collection of probabilities
\begin{align*}
P(Y = y_j| X = x) = \frac{P(Y = y_j \text{ and } X = x)}{P(X = x)}
\end{align*}
for all pairs of values $(x,y_j)$. 
\end{onebox}

::: {.workedexample data-latex=""}

```{r}
#| include: false
terms_chp_4 <- c(terms_chp_4, 
               c("joint distribution",
                 "conditional distribution",
                 "marginal distribution")
)
```

\(a\) Find the conditional probability that the number of prescriptions is 2 for a plan member who had 1 visit. (b) How does it compare to the marginal probability that $Y = 2$?  (c) What is the complete conditional distribution of $Y$, given $X = 1$?

------------------------------------------------------------------------
\(a)
\begin{align*}
  P(Y = 2 \mid X = 1) &= \frac{P(Y = 2 \text{ and } X = 1)} {P(X = 1)} \\
     &= 0.05/0.30 \\
     &= 0.167
\end{align*}

\(b\) The marginal probability that $Y = 2$ is 0.34.  Participants with only one visit are less likely to have 2 prescriptions filled than plan members generally.

\(c\) The complete conditional distribution consists of
\begin{align*}
P(Y = 0 \mid X = 1) &= \frac{P(Y = 0 \text{ and } X = 1)} {P(X = 1)} \\
      &= 0.07/0.30 \\
      &= 0.233, \\
P(Y = 1 \mid X = 1) &= \frac{P(Y = 1 \text{ and } X = 1)} {P(X = 1)} \\
      &= 0.18/0.30 \\
      &= 0.60, \\
P(Y = 2 \mid X = 1) &= \frac{P(Y = 2 \text{ and } X = 1)} {P(X = 1)} \\
      &= 0.05/0.30 \\
      &= 0.167
\end{align*}

  If a plan member has and one primary care visit, they are most likely to have a single prescription filled.
    
:::

A conditional distribution has the same properties as a full distribution; conditional probabilities are between 0 and 1, and they sum to 1, as in the above example.  

::: {.guidedpractice data-latex=""}

What is the average number of prescriptions filled among plan members with 1 primary care visit in a year? [^random-variables-710]
:::

[^random-variables-710]: The average will be the conditional mean of $Y$, given $X = 1$, calculated using the conditional distribution of $Y$, given $X = 1$, written as $E(Y \mid X = 1)$.
\begin{align*}
E(Y \mid X = 1) &= 0 P(Y = 0 \mid X = 1) + 1 P(Y = 1 \mid X = 1) + 2 P(Y = 2 \mid X = 1) \\
    &= (0)(0.233) + (1)(0.60) + (2)(0.167) \\
    &= 0.934.
\end{align*}


::: {.guidedpractice data-latex=""}

Consider two random variables, $X$ and $Y$, with the joint distribution shown in @tbl-joint-dist-gprac. 
\(a\) Compute the marginal distributions of $X$ and $Y$. 

\(b\) Identify the joint probability $p_{X,Y}(1, 2)$. 

\(c\) What is the value of $p_{X, Y}(2, 1)$? 

\(d\) Compute the conditional distribution of $X$ given that $Y = 2$.
[^random-variables-701]
:::

[^random-variables-701]:\(a\) The marginal distribution of $X$: $p_X(1) = 0.60$, $p_X(4) = 0.40$. The marginal distribution of $Y$: $p_Y(1) = 0.80$, $p_Y(2) = 0.20$ (b) $p_{X,Y}(1, 2) = P(X = 1, Y = 2) = 0.10$ (c) Since $X$ cannot take on value 2, $p_{X, Y}(2, 1) = 0$. (d) The conditional distribution of $X$ given that $Y = 2$: $p_{X|Y}(1|2) = \frac{p_{X, Y}(1, 2)}{p_Y(2)}  = \frac{0.10}{0.50} = 0.20$, $p_{X|Y}(4|2) = \frac{p_{X, Y}(4, 2)}{p_Y(2)}  = \frac{0.10}{0.20} = 0.50$.

|        | $Y = 1$ | $Y = 2$ |
|:------:|:------:|:------:|
| $X = 1$ | 0.50 | 0.10 |
| $X = 4$ | 0.30 | 0.10 |

: Joint distribution of $X$ and $Y$. {#tbl-joint-dist-gprac}

::: {.workedexample data-latex=""}

Compute the mean and standard deviation for $X$ and $Y$.

------------------------------------------------------------------------


$$
E(X)
= 1(0.60) + 4(0.40)
= 2.20
$$ 

$$
E(Y)
= 1(0.80) + 2(0.20)
= 1.20
$$ 

$$
E(X^2)
= 1^2(0.60) + 4^2(0.40)
= 1(0.60) + 16(0.40)
= 7.00
$$ 

$$
\operatorname{Var}(X)
= E(X^2) - [E(X)]^2
= 7.00 - (2.20)^2
= 2.16
$$ 

$$
\sigma_X
= \sqrt{\operatorname{Var}(X)}
= \sqrt{2.16}
\approx 1.47
$$ 

$$
E(Y^2)
= 1^2(0.80) + 2^2(0.20)
= 1(0.80) + 4(0.20)
= 1.60
$$ 

$$
\operatorname{Var}(Y)
= E(Y^2) - [E(Y)]^2
= 1.60 - (1.20)^2
= 0.16
$$ 

$$
\sigma_Y
= \sqrt{\operatorname{Var}(Y)}
= \sqrt{0.16}
= 0.40
$$ 


:::

### Independent random variables

Two events $A$ and $B$ are independent if $P(B|A) = P(B)$ (@eq-independence-definition) and this definition can be extended to random variables. The variables $X$ and $Y$ are called  **independent random variables** \index{independent random variables} if the conditional probability of any outcome for $Y$ does not change given any outcome of $X$. 

::: {.important data-latex=""}
**Independent random variables.**

Two random variables $X$ and $Y$ are independent if the probabilities
\begin{align*}
P(Y = y_j| X = x_i) = P(Y = y_j)
\end{align*}
for all pairs of values $(x_i,y_j)$. 
Equivalently, $X$ and $Y$ are independent if the probabilities
\begin{align*}
P(Y = y_j \text{ and } X = x_i) = P(Y = y_j)P(X = x_i)
\end{align*}
for all pairs of values $(x_i,y_j)$.

:::

Independence is a strong assumption that can be hard to justify, and there are many settings where random variables are not independent.  Height and weight of an individual sampled from a population are not independent -- taller people tend to weigh more.  The lowest temperature in a day is not independent of the number of daylight hours -- the shorter days in winter tend to be colder than days in other seasons. Independent random variables do arise in some experimental designs or surveys.  The responses of two participants in a randomized treatment experiment will be independent, since the response of one patient will not affect the response of the other.  The SAT scores of two high school students living in different parts of the country are independent -- knowing the score of one does not provide information about the other score when nothing else is known about the students.  When two dice are rolled, the values showing on the two faces are independent. 

Sometimes whether or not two random variables are independent is clear from the context, but the best way to check for independence is from the joint distribution, when it is available.

::: {.guidedpractice data-latex=""}

Are the random variables $X$ and $Y$ with joint distribution in @tbl-joint-dist-gprac independent?
[^random-variables-702]
:::

[^random-variables-702]:  No, since $p_{X|Y}(4|2) = \frac{p_{X, Y}(4, 2)}{p_Y(2)}  = \frac{0.10}{0.20} = 0.50$, but $P(X = 4) = 0.40$. 

### Correlated random variables {#sec-correlated-rv}

Two events $A$ and $B$ are independent if $P(B|A) = P(B)$ (@eq-independence-definition) and dependent when $P(B\mid A) \neq P(B)$.  Suppose an individual is sampled from the GSS data described in  @sec-gss-example, and let $A$ = {sampled individual is age 18 - 25} and $B$ = {health condition excellent}. $A$ and $B$ are not independent because from @tbl-gss-age-health-prop, $P(B) = 0.197$ but @tbl-gss-age-health-row-prop shows that $P(B|A) = 0.275$.  When $A$ is known to have occurred, the probability of $B$ is updated from $P(B)$ to $P(B \mid A)$.    

Two random variables that are not independent are called \index{correlated random variables} **correlated random variables**. The correlation between two random variables is a measure of the strength of the association between them, just as it was for pairs of data points explored in @sec-relationships-two-numerical.  There are many examples of correlated random variables, such as height and weight in a population of individuals, or the gestational age and birth weight of newborns.  
When two random variables are positively correlated, they tend to increase or decrease together. If one of the variables increases while the other decreases (or vice versa) they are negatively correlated.  Correlation is easy to identify in a scatterplot, but is more difficult to identify in a table of a joint distribution.  Fortunately, there is a formula to calculate correlation for a joint distribution specified  in a table.

Correlation between random variables is similar to correlation between pairs of observations in a dataset, with some important differences.  Calculating a correlation $r$ in a dataset was introduced in @sec-relationships-two-numerical, @eq-correlation-equation:
\begin{align*}
r &=  \frac{1}{n-1}\sum^{n}_{i=1}
\left(\frac{x_{i}-\overline{x}}
{s_{x}}\right)\left(\frac{y_{i}-\overline{y}}{s_{y}}\right).
\end{align*} 
The correlation coefficient $r$ is an average of products, with each term in the product measuring the distance between $x$ and its mean $\overline{x}$ and the distance between $y$ and its mean $\overline{y}$, after the distances have been scaled by respective standard deviations.

The compact formula for the correlation between two random variables $X$ and $Y$  uses the same idea.

::: {.important data-latex=""}
**Correlation of random variables.**

The correlation $\rho_{X,Y}$ between two random variables $X$ and $Y$ is given by

$$
\rho_{X,Y} = E\left(\frac{X - \mu_x}{\sigma_X}\right)\left(\frac{Y - \mu_Y}{\sigma_Y} \right),
$${#eq-gen-form-corr}
where  $\mu_X, \mu_Y$, $\sigma_X, \sigma_Y$ are the respective means and standard deviations for $X$ and $Y$. 

When $X$ and $Y$ are independent, $\rho_{X,Y} = 0$.

:::

Just as with the mean of a single random variable, the expectation is a weighted sum of the products, with each term weighted by the probability  $p(x_i, y_j)$ of the values for the pair $(X,Y)$.  @eq-gen-form-corr is useful for understanding the analogy between correlation of random variables and correlation of observations in a dataset, but it cannot be used to calculate $\rho_{X,Y}$ without the probability weights. 

@eq-spec-form-corr is an expansion of @eq-gen-form-corr. The double summation adds up terms over all combinations of the indices $i$ and $j$.

$$
\rho_{X,Y} = \sum_{i} \sum_{j} p(x_i,y_j)\frac{(x_i - \mu_X)}{\sigma_X}\frac{(y_j - \mu_Y)}{\sigma_Y}.
$${#eq-spec-form-corr}

Because the two standard deviations are constants, @eq-spec-form-corr can be written as @eq-spec-form-corr-alt:

$$
\rho_{X,Y} = \frac{\sum_{i} \sum_{j} p(x_i,y_j)(x_i - \mu_X)(y_j - \mu_Y)}{\sigma_X \sigma_Y}.
$$ {#eq-spec-form-corr-alt}

::: {.workedexample data-latex=""}

Calculate the correlation of the variables $X$ and $Y$ with joint distribution in @tbl-joint-dist-gprac, using @eq-spec-form-corr-alt.

------------------------------------------------------------------------



The means and standard deviations have already been calculated:
\begin{align*}
\mu_X &= 2.20, \\
\mu_Y &= 1.20, \\
\sigma_X &= 1.47,  \\
\sigma_Y &= 0.40.
\end{align*}

Now compute the numerator:
\begin{align*}
E[(X-\mu_X)(Y-\mu_Y)]
&= \sum_{x,y} (x-2.20)(y-1.20)\,p(x,y) \\[1mm]
&= (1-2.20)(1-1.20)(0.50)
 + (1-2.20)(2-1.20)(0.10) \\
&\quad + (4-2.20)(1-1.20)(0.30)
 + (4-2.20)(2-1.20)(0.10) \\[1mm]
&= (-1.20)(-0.20)(0.50)
 + (-1.20)(0.80)(0.10) \\
&\quad + (1.80)(-0.20)(0.30)
 + (1.80)(0.80)(0.10) \\[1mm]
&= 0.12 - 0.096 - 0.108 + 0.144 \\[1mm]
&= 0.06.
\end{align*}


Finally,
\begin{align*}
\rho_{X,Y}
&= \frac{0.06}{(1.47)(0.40)} \\
&= 0.10.
\end{align*}

The variables $X$ and $Y$ are positively correlated.

:::

```{r}
#| include: false
terms_chp_4 <- c(terms_chp_4, 
               c("independent random variables",
                 "correlated random variables")
)
```

::: {.guidedpractice data-latex=""}

Use @tbl-joint-dist-gprac to explain why $X$ and $Y$ are positively correlated. [^random-variables-703]
:::

[^random-variables-703]:  When $X = 4$ it is larger than its mean $\mu_X = 2.20 $ and $Y$ is more likely to be larger than its mean $\mu_Y = 1.20$, and vice versa.

When $X$ and $Y$ are independent, $\operatorname{Var}(X + Y) = \operatorname{Var}(X) + \operatorname{Var(Y)}.$ (@eq-variance-linear-combination-ind-rv in @sec-sum-indep-rvs)

When two random variables $X$ and $Y$ are correlated:

$$
\operatorname{Var}(X + Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) + 
2 \sigma_X \sigma_Y \rho_{X,Y} 
$$ {#eq-general-var-sum-rvs}


$$\text{Var}(X - Y) = \text{Var}(X) + \text{Var}(Y) - 
2 \sigma_X \sigma_Y \rho_{X,Y}. 
$$ {#eq-general-var-diff-rvs}


When random variables are positively correlated the variance of the sum will be larger than the sum of the two variances, and the variance of the difference will be smaller than the sum of the two variances. When they are negatively correlated the variance of the sum will be smaller than the sum of the two variances. 

In either case, the standard deviation for the sum or difference is the square root of the variance.


When $X$ and $Y$ are correlated, it is still the case that
\begin{align*}
E(X + Y) &= E(X) + E(Y) \\
E(X - Y) &= E(X) - E(Y).
\end{align*}

::: {.workedexample data-latex=""}

Using @eq-general-var-sum-rvs Compute the variance and standard deviation of $X + Y$ if $X$ and $Y$ have the joint distribution in @tbl-joint-dist-gprac.  Compare the variance of $X + Y$ with $\operatorname{Var}(X) + \operatorname{Var}(Y)$ and explain the difference.

------------------------------------------------------------------------

\begin{align*}
  \operatorname{Var}(X + Y) &= \operatorname{Var}(X) + \operatorname{Var}(Y) + 
2 \sigma_X \sigma_Y \rho_{X,Y} \\
  &= 2.16 + 0.50 - (1.47)(0.25)(0.408) \\
  &= 2.51, \\
\sigma_{X + Y} &= \sqrt{2.51} \\
               &= 1.58.  
\end{align*}
The $\operatorname{Var}(X + Y) > \operatorname{Var}(X) + \operatorname{Var}{Y}$ because $X$ and $Y$ are positively correlated.

:::

::: {.workedexample data-latex=""}

The Association of American Medical Colleges (AAMC) introduced a new version of the Medical College Admission Test (MCAT) in the spring of 2015. Data from the scores were recently released by AAMC.^[\url{https://www.aamc.org/students/download/434504/data/percentilenewmcat.pdf}] The test consists of 4 components: chemical and physical foundations of biological systems; critical analysis and reasoning skills; biological and biochemical foundations of living systems; psychological, social and biological foundations of behavior. The overall score is the sum of the individual component scores. The grading for each of the four components is scaled so that the mean score is 125.  The means and standard deviations for the four components and the total scores for the population taking the exam in May 2015 exam are shown in @tbl-mcat-score-distribution.

Show that the standard deviation in the table for the total score does not agree with that obtained under the assumption of independence.

| Component          | Mean | Standard Deviation   |
|:------------------------|:--------:|:----------------------:|
| Chem. Phys. Found.     | 125      | 3.0 |
| Crit. Analysis         | 125      | 3.0 |
| Living Systems         | 125      | 3.0 |
| Found. Behavior        | 125      | 3.1 |
| **Total Score**        | **500**  | **10.6** |

: Means and standard deviations for MCAT Scores {#tbl-mcat-score-distribution}

------------------------------------------------------------------------

The variance of each component of the score is the square of each standard deviation.  Under the assumption of independence, the variance of the total score would be 
\begin{align*}
	\operatorname{Var}(\text{Total Score}) &= 3.0^2 + 3.0^2 + 3.0^2 + 3.1^2 \\
	   &= 36.61,
\end{align*}
	so the standard deviation is 6.05, which is less than 10.6. 
	
Since the observed standard deviation is larger than that calculated under independence, this suggests the component scores are positively correlated.  It would not be reasonable to expect that the component scores are independent. Think about a student taking the MCAT exam: someone who scores well on one component of the exam is likely to score well on the other parts.

:::

```{r}
#| include: false
terms_chp_4 <- c(terms_chp_4, 
               c("variance of sum of correlated random variables")
)
```

## Chapter review {#sec-chp4-review}



### Summary

Random variables are an important link between exploring a dataset and learning about the phenomenon that was the source of the data.  Observations made on participants in a study are never constant -- outcomes vary from participant to participant. Indeed, when studies are repeated, even the summary statistics such as a  mean or the variability of responses will differ.  Random variables model that variability by thinking of data as observations on random variables.  They are a way of thinking about the potential outcomes of an observation,

The distribution of a random variable characterizes the range of values that might be observed and which values have high or low  likelihood.  Those distributions have means, called expected values, and a variability captured by standard deviations. In data, the mean is the center of the distribution that the data analyst is examining.  For a random variable, the mean is the expected value of the next observation.  

Two random variables are independent when the observation of 1 provides no information about the distribution of the other;  when they are not independent, the association between the pair is estimated by their correlation. 

The material in this chapter serves as a background to the important distributions described in the next chapter, distributions and random variables that are  widely used in the life sciences, public health and medical research.



### Terms


```{r}
#| label: tbl-terms-chp-4
#| tbl-cap: Terms introduced in this chapter.
#| tbl-pos: H
make_terms_table(terms_chp_4)
```

\clearpage

## Exercises {#sec-chp4-exercises}

The exercises are not numbered yet and no solutions are available in this draft.

<!---
Answers to odd-numbered exercises can be found in [Appendix -@sec-exercise-solutions-04].
--->
::: {.exercises data-latex=""}
{{< include exercises/_04-ex-random-variables.qmd >}}
:::


